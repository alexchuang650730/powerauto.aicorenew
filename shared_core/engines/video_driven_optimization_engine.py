#!/usr/bin/env python3
"""
PowerAutomation v0.575 - è¦–é »é©…å‹•æ¸¬è©¦ç”¨ä¾‹å„ªåŒ–ç³»çµ±

æ ¸å¿ƒæŠ€è¡“é›£é»2è§£æ±ºæ–¹æ¡ˆï¼š
å¦‚ä½•åˆ©ç”¨ç”¢ç”Ÿçš„è¦–é »å’Œå·¥ä½œæ–‡æœ¬ä¾†å”åŠ©ä¿®æ­£åŠç´°åŒ–ï¼Œç”¢ç”Ÿæ›´ç´°æ›´æ­£ç¢ºæ›´å¤šæ¨£çš„test cases

æŠ€è¡“æ¶æ§‹ï¼š
1. è¦–é »åˆ†æå¼•æ“ - è§£æåŸ·è¡Œéç¨‹è¦–é »ï¼Œæå–é—œéµå¹€å’Œæ“ä½œåºåˆ—
2. è¡Œç‚ºè­˜åˆ¥ç³»çµ± - è­˜åˆ¥ç”¨æˆ¶æ“ä½œã€ç³»çµ±éŸ¿æ‡‰ã€éŒ¯èª¤ç‹€æ…‹ç­‰
3. æ–‡æœ¬è¦–é »é—œè¯å™¨ - å°‡å·¥ä½œæ–‡æœ¬èˆ‡è¦–é »ç‰‡æ®µé€²è¡Œæ™‚é–“è»¸é—œè¯
4. æ¸¬è©¦ç”¨ä¾‹ç´°åŒ–å™¨ - åŸºæ–¼è¦–é »åˆ†æçµæœå„ªåŒ–å’Œç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
5. åé¥‹å­¸ç¿’æ©Ÿåˆ¶ - å¾è¦–é »é©—è­‰çµæœä¸­å­¸ç¿’æ”¹é€²
"""

import os
import cv2
import json
import time
import base64
import hashlib
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import pickle
from collections import defaultdict, deque
import threading
import asyncio
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VideoEventType(Enum):
    """è¦–é »äº‹ä»¶é¡å‹"""
    CLICK = "click"                    # é»æ“Šäº‹ä»¶
    INPUT = "input"                    # è¼¸å…¥äº‹ä»¶
    SCROLL = "scroll"                  # æ»¾å‹•äº‹ä»¶
    NAVIGATION = "navigation"          # å°èˆªäº‹ä»¶
    WAIT = "wait"                      # ç­‰å¾…äº‹ä»¶
    ERROR = "error"                    # éŒ¯èª¤äº‹ä»¶
    SUCCESS = "success"                # æˆåŠŸäº‹ä»¶
    VALIDATION = "validation"          # é©—è­‰äº‹ä»¶
    SCREENSHOT = "screenshot"          # æˆªåœ–äº‹ä»¶

class TestCaseType(Enum):
    """æ¸¬è©¦ç”¨ä¾‹é¡å‹"""
    FUNCTIONAL = "functional"          # åŠŸèƒ½æ¸¬è©¦
    UI_INTERACTION = "ui_interaction"  # UIäº¤äº’æ¸¬è©¦
    ERROR_HANDLING = "error_handling"  # éŒ¯èª¤è™•ç†æ¸¬è©¦
    PERFORMANCE = "performance"        # æ€§èƒ½æ¸¬è©¦
    REGRESSION = "regression"          # å›æ­¸æ¸¬è©¦
    EDGE_CASE = "edge_case"           # é‚Šç•Œæƒ…æ³æ¸¬è©¦

class VideoQuality(Enum):
    """è¦–é »è³ªé‡ç­‰ç´š"""
    HIGH = "high"      # é«˜è³ªé‡ï¼šæ¸…æ™°ã€å®Œæ•´ã€ç„¡éŒ¯èª¤
    MEDIUM = "medium"  # ä¸­ç­‰è³ªé‡ï¼šåŸºæœ¬æ¸…æ™°ã€å¤§éƒ¨åˆ†å®Œæ•´
    LOW = "low"        # ä½è³ªé‡ï¼šæ¨¡ç³Šã€ä¸å®Œæ•´ã€æœ‰éŒ¯èª¤

@dataclass
class VideoFrame:
    """è¦–é »å¹€æ•¸æ“š"""
    frame_id: str
    timestamp: float
    frame_data: np.ndarray
    frame_hash: str
    annotations: List[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.annotations is None:
            self.annotations = []

@dataclass
class VideoEvent:
    """è¦–é »äº‹ä»¶"""
    event_id: str
    event_type: VideoEventType
    timestamp: float
    duration: float
    coordinates: Optional[Tuple[int, int]] = None
    text_content: Optional[str] = None
    element_info: Optional[Dict[str, Any]] = None
    screenshot_before: Optional[str] = None
    screenshot_after: Optional[str] = None
    confidence: float = 0.0
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class WorkflowStep:
    """å·¥ä½œæµæ­¥é©Ÿ"""
    step_id: str
    step_number: int
    description: str
    expected_result: str
    actual_result: Optional[str] = None
    video_events: List[VideoEvent] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    status: str = "pending"  # pending, running, completed, failed
    
    def __post_init__(self):
        if self.video_events is None:
            self.video_events = []

@dataclass
class VideoAnalysisResult:
    """è¦–é »åˆ†æçµæœ"""
    video_id: str
    video_path: str
    analysis_timestamp: datetime
    total_duration: float
    frame_count: int
    fps: float
    resolution: Tuple[int, int]
    quality_score: float
    detected_events: List[VideoEvent]
    workflow_steps: List[WorkflowStep]
    error_segments: List[Dict[str, Any]]
    performance_metrics: Dict[str, float]
    text_video_correlations: List[Dict[str, Any]]
    
class VideoAnalysisEngine:
    """è¦–é »åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.frame_cache = {}
        self.event_patterns = self._load_event_patterns()
        self.ui_element_detector = UIElementDetector()
        self.text_extractor = VideoTextExtractor()
        
    def _load_event_patterns(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è¼‰äº‹ä»¶è­˜åˆ¥æ¨¡å¼"""
        return {
            "click_pattern": {
                "color_change_threshold": 30,
                "duration_range": (0.1, 2.0),
                "cursor_movement_threshold": 10
            },
            "input_pattern": {
                "text_change_detection": True,
                "cursor_blink_detection": True,
                "keyboard_activity_threshold": 0.5
            },
            "scroll_pattern": {
                "content_shift_threshold": 20,
                "scroll_bar_detection": True,
                "duration_range": (0.2, 5.0)
            },
            "error_pattern": {
                "error_color_signatures": [(255, 0, 0), (255, 165, 0)],  # ç´…è‰²ã€æ©™è‰²
                "error_text_keywords": ["error", "éŒ¯èª¤", "å¤±æ•—", "exception", "failed"],
                "popup_detection": True
            }
        }
    
    def analyze_video(self, video_path: str, workflow_text: str = "") -> VideoAnalysisResult:
        """åˆ†æè¦–é »ä¸¦æå–äº‹ä»¶"""
        logger.info(f"ğŸ¬ é–‹å§‹åˆ†æè¦–é »: {video_path}")
        
        # æ‰“é–‹è¦–é »æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"ç„¡æ³•æ‰“é–‹è¦–é »æ–‡ä»¶: {video_path}")
        
        # ç²å–è¦–é »åŸºæœ¬ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_duration = frame_count / fps if fps > 0 else 0
        
        logger.info(f"ğŸ“Š è¦–é »ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_duration:.2f}s, {frame_count}å¹€")
        
        # åˆ†æè¦–é »å¹€
        frames = self._extract_frames(cap, fps)
        detected_events = self._detect_events(frames, fps)
        workflow_steps = self._correlate_with_workflow(detected_events, workflow_text)
        error_segments = self._detect_error_segments(frames, detected_events)
        performance_metrics = self._calculate_performance_metrics(detected_events, total_duration)
        text_correlations = self._correlate_text_video(workflow_text, detected_events)
        quality_score = self._calculate_video_quality(frames, detected_events)
        
        cap.release()
        
        # å‰µå»ºåˆ†æçµæœ
        result = VideoAnalysisResult(
            video_id=hashlib.md5(video_path.encode()).hexdigest()[:12],
            video_path=video_path,
            analysis_timestamp=datetime.now(),
            total_duration=total_duration,
            frame_count=frame_count,
            fps=fps,
            resolution=(width, height),
            quality_score=quality_score,
            detected_events=detected_events,
            workflow_steps=workflow_steps,
            error_segments=error_segments,
            performance_metrics=performance_metrics,
            text_video_correlations=text_correlations
        )
        
        logger.info(f"âœ… è¦–é »åˆ†æå®Œæˆï¼Œæª¢æ¸¬åˆ° {len(detected_events)} å€‹äº‹ä»¶")
        return result
    
    def _extract_frames(self, cap: cv2.VideoCapture, fps: float) -> List[VideoFrame]:
        """æå–é—œéµå¹€"""
        frames = []
        frame_id = 0
        
        # æ¯ç§’æå–2-4å¹€ï¼Œæ ¹æ“šfpsèª¿æ•´
        sample_rate = max(1, int(fps / 3))
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_id % sample_rate == 0:
                timestamp = frame_id / fps
                frame_hash = hashlib.md5(frame.tobytes()).hexdigest()[:16]
                
                video_frame = VideoFrame(
                    frame_id=f"frame_{frame_id:06d}",
                    timestamp=timestamp,
                    frame_data=frame.copy(),
                    frame_hash=frame_hash
                )
                frames.append(video_frame)
            
            frame_id += 1
        
        logger.info(f"ğŸ“¸ æå–äº† {len(frames)} å€‹é—œéµå¹€")
        return frames
    
    def _detect_events(self, frames: List[VideoFrame], fps: float) -> List[VideoEvent]:
        """æª¢æ¸¬è¦–é »äº‹ä»¶"""
        events = []
        
        for i in range(1, len(frames)):
            prev_frame = frames[i-1]
            curr_frame = frames[i]
            
            # æª¢æ¸¬é»æ“Šäº‹ä»¶
            click_events = self._detect_click_events(prev_frame, curr_frame)
            events.extend(click_events)
            
            # æª¢æ¸¬è¼¸å…¥äº‹ä»¶
            input_events = self._detect_input_events(prev_frame, curr_frame)
            events.extend(input_events)
            
            # æª¢æ¸¬æ»¾å‹•äº‹ä»¶
            scroll_events = self._detect_scroll_events(prev_frame, curr_frame)
            events.extend(scroll_events)
            
            # æª¢æ¸¬éŒ¯èª¤äº‹ä»¶
            error_events = self._detect_error_events(prev_frame, curr_frame)
            events.extend(error_events)
            
            # æª¢æ¸¬å°èˆªäº‹ä»¶
            nav_events = self._detect_navigation_events(prev_frame, curr_frame)
            events.extend(nav_events)
        
        # æŒ‰æ™‚é–“æ’åºäº‹ä»¶
        events.sort(key=lambda e: e.timestamp)
        
        logger.info(f"ğŸ” æª¢æ¸¬åˆ° {len(events)} å€‹äº‹ä»¶")
        return events
    
    def _detect_click_events(self, prev_frame: VideoFrame, curr_frame: VideoFrame) -> List[VideoEvent]:
        """æª¢æ¸¬é»æ“Šäº‹ä»¶"""
        events = []
        
        # è¨ˆç®—å¹€å·®ç•°
        diff = cv2.absdiff(prev_frame.frame_data, curr_frame.frame_data)
        gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        
        # æŸ¥æ‰¾é¡¯è‘—è®ŠåŒ–å€åŸŸ
        _, thresh = cv2.threshold(gray_diff, 30, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 100 < area < 5000:  # é»æ“Šå€åŸŸå¤§å°ç¯„åœ
                # ç²å–é‚Šç•Œæ¡†
                x, y, w, h = cv2.boundingRect(contour)
                center_x, center_y = x + w//2, y + h//2
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé»æ“Šæ¨¡å¼
                if self._is_click_pattern(prev_frame.frame_data, curr_frame.frame_data, (center_x, center_y)):
                    event = VideoEvent(
                        event_id=f"click_{curr_frame.frame_id}_{center_x}_{center_y}",
                        event_type=VideoEventType.CLICK,
                        timestamp=curr_frame.timestamp,
                        duration=curr_frame.timestamp - prev_frame.timestamp,
                        coordinates=(center_x, center_y),
                        confidence=0.7,
                        metadata={"area": area, "contour_points": len(contour)}
                    )
                    events.append(event)
        
        return events
    
    def _detect_input_events(self, prev_frame: VideoFrame, curr_frame: VideoFrame) -> List[VideoEvent]:
        """æª¢æ¸¬è¼¸å…¥äº‹ä»¶"""
        events = []
        
        # æå–æ–‡æœ¬å€åŸŸ
        prev_text = self.text_extractor.extract_text(prev_frame.frame_data)
        curr_text = self.text_extractor.extract_text(curr_frame.frame_data)
        
        # æ¯”è¼ƒæ–‡æœ¬è®ŠåŒ–
        if prev_text != curr_text:
            # æ‰¾å‡ºè®ŠåŒ–çš„æ–‡æœ¬å€åŸŸ
            text_changes = self._find_text_changes(prev_text, curr_text)
            
            for change in text_changes:
                event = VideoEvent(
                    event_id=f"input_{curr_frame.frame_id}_{change['position']}",
                    event_type=VideoEventType.INPUT,
                    timestamp=curr_frame.timestamp,
                    duration=curr_frame.timestamp - prev_frame.timestamp,
                    text_content=change['new_text'],
                    coordinates=change.get('coordinates'),
                    confidence=0.8,
                    metadata={"change_type": change['type'], "old_text": change['old_text']}
                )
                events.append(event)
        
        return events
    
    def _detect_scroll_events(self, prev_frame: VideoFrame, curr_frame: VideoFrame) -> List[VideoEvent]:
        """æª¢æ¸¬æ»¾å‹•äº‹ä»¶"""
        events = []
        
        # æª¢æ¸¬å‚ç›´å…§å®¹ç§»å‹•
        prev_gray = cv2.cvtColor(prev_frame.frame_data, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(curr_frame.frame_data, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å…‰æµæª¢æ¸¬é‹å‹•
        flow = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, None, None)
        
        if flow is not None:
            # åˆ†æé‹å‹•å‘é‡
            vertical_movement = self._analyze_vertical_movement(flow)
            
            if abs(vertical_movement) > 20:  # æ»¾å‹•é–¾å€¼
                direction = "down" if vertical_movement > 0 else "up"
                
                event = VideoEvent(
                    event_id=f"scroll_{curr_frame.frame_id}_{direction}",
                    event_type=VideoEventType.SCROLL,
                    timestamp=curr_frame.timestamp,
                    duration=curr_frame.timestamp - prev_frame.timestamp,
                    confidence=0.6,
                    metadata={"direction": direction, "distance": abs(vertical_movement)}
                )
                events.append(event)
        
        return events
    
    def _detect_error_events(self, prev_frame: VideoFrame, curr_frame: VideoFrame) -> List[VideoEvent]:
        """æª¢æ¸¬éŒ¯èª¤äº‹ä»¶"""
        events = []
        
        # æª¢æ¸¬éŒ¯èª¤é¡è‰²ç‰¹å¾µ
        error_regions = self._detect_error_colors(curr_frame.frame_data)
        
        # æª¢æ¸¬éŒ¯èª¤æ–‡æœ¬
        frame_text = self.text_extractor.extract_text(curr_frame.frame_data)
        error_texts = self._detect_error_text(frame_text)
        
        # æª¢æ¸¬å½ˆçª—
        popup_detected = self._detect_popup(prev_frame.frame_data, curr_frame.frame_data)
        
        if error_regions or error_texts or popup_detected:
            event = VideoEvent(
                event_id=f"error_{curr_frame.frame_id}",
                event_type=VideoEventType.ERROR,
                timestamp=curr_frame.timestamp,
                duration=curr_frame.timestamp - prev_frame.timestamp,
                text_content="; ".join(error_texts) if error_texts else None,
                confidence=0.8,
                metadata={
                    "error_regions": len(error_regions),
                    "error_texts": error_texts,
                    "popup_detected": popup_detected
                }
            )
            events.append(event)
        
        return events
    
    def _detect_navigation_events(self, prev_frame: VideoFrame, curr_frame: VideoFrame) -> List[VideoEvent]:
        """æª¢æ¸¬å°èˆªäº‹ä»¶"""
        events = []
        
        # æª¢æ¸¬URLè®ŠåŒ–ï¼ˆå¦‚æœæœ‰åœ°å€æ¬„ï¼‰
        prev_text = self.text_extractor.extract_text(prev_frame.frame_data)
        curr_text = self.text_extractor.extract_text(curr_frame.frame_data)
        
        # ç°¡å–®çš„URLæª¢æ¸¬
        prev_urls = self._extract_urls(prev_text)
        curr_urls = self._extract_urls(curr_text)
        
        if prev_urls != curr_urls:
            event = VideoEvent(
                event_id=f"nav_{curr_frame.frame_id}",
                event_type=VideoEventType.NAVIGATION,
                timestamp=curr_frame.timestamp,
                duration=curr_frame.timestamp - prev_frame.timestamp,
                confidence=0.7,
                metadata={"prev_urls": prev_urls, "curr_urls": curr_urls}
            )
            events.append(event)
        
        return events
    
    def _is_click_pattern(self, prev_frame: np.ndarray, curr_frame: np.ndarray, 
                         coordinates: Tuple[int, int]) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºé»æ“Šæ¨¡å¼"""
        x, y = coordinates
        
        # æª¢æŸ¥é»æ“Šå€åŸŸçš„é¡è‰²è®ŠåŒ–
        region_size = 20
        x1, y1 = max(0, x-region_size), max(0, y-region_size)
        x2, y2 = min(prev_frame.shape[1], x+region_size), min(prev_frame.shape[0], y+region_size)
        
        prev_region = prev_frame[y1:y2, x1:x2]
        curr_region = curr_frame[y1:y2, x1:x2]
        
        if prev_region.size == 0 or curr_region.size == 0:
            return False
        
        # è¨ˆç®—é¡è‰²å·®ç•°
        color_diff = np.mean(cv2.absdiff(prev_region, curr_region))
        
        return color_diff > 30  # é¡è‰²è®ŠåŒ–é–¾å€¼
    
    def _find_text_changes(self, prev_text: str, curr_text: str) -> List[Dict[str, Any]]:
        """æ‰¾å‡ºæ–‡æœ¬è®ŠåŒ–"""
        changes = []
        
        # ç°¡å–®çš„æ–‡æœ¬å·®ç•°æª¢æ¸¬
        if len(curr_text) > len(prev_text):
            # æ–‡æœ¬å¢åŠ 
            new_text = curr_text[len(prev_text):]
            changes.append({
                "type": "addition",
                "old_text": prev_text,
                "new_text": new_text,
                "position": len(prev_text)
            })
        elif len(curr_text) < len(prev_text):
            # æ–‡æœ¬åˆªé™¤
            changes.append({
                "type": "deletion",
                "old_text": prev_text[len(curr_text):],
                "new_text": "",
                "position": len(curr_text)
            })
        elif prev_text != curr_text:
            # æ–‡æœ¬ä¿®æ”¹
            changes.append({
                "type": "modification",
                "old_text": prev_text,
                "new_text": curr_text,
                "position": 0
            })
        
        return changes
    
    def _analyze_vertical_movement(self, flow) -> float:
        """åˆ†æå‚ç›´é‹å‹•"""
        # é€™è£¡éœ€è¦å¯¦éš›çš„å…‰æµåˆ†æå¯¦ç¾
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šè¿”å›éš¨æ©Ÿå€¼æ¨¡æ“¬
        return np.random.uniform(-50, 50)
    
    def _detect_error_colors(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """æª¢æ¸¬éŒ¯èª¤é¡è‰²"""
        error_regions = []
        
        # æª¢æ¸¬ç´…è‰²å€åŸŸ
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # ç´…è‰²ç¯„åœ
        lower_red1 = np.array([0, 50, 50])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([170, 50, 50])
        upper_red2 = np.array([180, 255, 255])
        
        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        red_mask = mask1 + mask2
        
        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 100:  # æœ€å°éŒ¯èª¤å€åŸŸå¤§å°
                x, y, w, h = cv2.boundingRect(contour)
                error_regions.append({
                    "color": "red",
                    "area": area,
                    "bbox": (x, y, w, h)
                })
        
        return error_regions
    
    def _detect_error_text(self, text: str) -> List[str]:
        """æª¢æ¸¬éŒ¯èª¤æ–‡æœ¬"""
        error_keywords = [
            "error", "éŒ¯èª¤", "å¤±æ•—", "exception", "failed", "invalid", 
            "ç„¡æ•ˆ", "ä¸æ­£ç¢º", "timeout", "è¶…æ™‚", "denied", "æ‹’çµ•"
        ]
        
        found_errors = []
        text_lower = text.lower()
        
        for keyword in error_keywords:
            if keyword in text_lower:
                found_errors.append(keyword)
        
        return found_errors
    
    def _detect_popup(self, prev_frame: np.ndarray, curr_frame: np.ndarray) -> bool:
        """æª¢æ¸¬å½ˆçª—"""
        # ç°¡å–®çš„å½ˆçª—æª¢æ¸¬ï¼šæª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„çŸ©å½¢å€åŸŸå‡ºç¾
        diff = cv2.absdiff(prev_frame, curr_frame)
        gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        
        _, thresh = cv2.threshold(gray_diff, 50, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 10000:  # å½ˆçª—æœ€å°å¤§å°
                # æª¢æŸ¥æ˜¯å¦ç‚ºçŸ©å½¢
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)
                if len(approx) == 4:  # çŸ©å½¢
                    return True
        
        return False
    
    def _extract_urls(self, text: str) -> List[str]:
        """æå–URL"""
        import re
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        return re.findall(url_pattern, text)
    
    def _correlate_with_workflow(self, events: List[VideoEvent], workflow_text: str) -> List[WorkflowStep]:
        """èˆ‡å·¥ä½œæµæ–‡æœ¬é—œè¯"""
        steps = []
        
        # è§£æå·¥ä½œæµæ–‡æœ¬
        workflow_lines = workflow_text.split('\n')
        step_descriptions = [line.strip() for line in workflow_lines if line.strip()]
        
        # å°‡äº‹ä»¶åˆ†çµ„åˆ°æ­¥é©Ÿä¸­
        events_per_step = len(events) // max(len(step_descriptions), 1) if step_descriptions else len(events)
        
        for i, description in enumerate(step_descriptions):
            start_idx = i * events_per_step
            end_idx = min((i + 1) * events_per_step, len(events))
            step_events = events[start_idx:end_idx]
            
            step = WorkflowStep(
                step_id=f"step_{i+1:03d}",
                step_number=i + 1,
                description=description,
                expected_result="æ­¥é©ŸæˆåŠŸå®Œæˆ",
                video_events=step_events,
                start_time=step_events[0].timestamp if step_events else None,
                end_time=step_events[-1].timestamp if step_events else None,
                status="completed" if step_events else "pending"
            )
            steps.append(step)
        
        return steps
    
    def _detect_error_segments(self, frames: List[VideoFrame], events: List[VideoEvent]) -> List[Dict[str, Any]]:
        """æª¢æ¸¬éŒ¯èª¤ç‰‡æ®µ"""
        error_segments = []
        
        error_events = [e for e in events if e.event_type == VideoEventType.ERROR]
        
        for error_event in error_events:
            segment = {
                "start_time": error_event.timestamp,
                "end_time": error_event.timestamp + error_event.duration,
                "error_type": "unknown",
                "description": error_event.text_content or "æª¢æ¸¬åˆ°éŒ¯èª¤",
                "confidence": error_event.confidence
            }
            error_segments.append(segment)
        
        return error_segments
    
    def _calculate_performance_metrics(self, events: List[VideoEvent], total_duration: float) -> Dict[str, float]:
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        metrics = {
            "total_events": len(events),
            "events_per_second": len(events) / total_duration if total_duration > 0 else 0,
            "click_events": len([e for e in events if e.event_type == VideoEventType.CLICK]),
            "input_events": len([e for e in events if e.event_type == VideoEventType.INPUT]),
            "error_events": len([e for e in events if e.event_type == VideoEventType.ERROR]),
            "average_event_duration": np.mean([e.duration for e in events]) if events else 0,
            "error_rate": len([e for e in events if e.event_type == VideoEventType.ERROR]) / len(events) if events else 0
        }
        
        return metrics
    
    def _correlate_text_video(self, workflow_text: str, events: List[VideoEvent]) -> List[Dict[str, Any]]:
        """é—œè¯æ–‡æœ¬å’Œè¦–é »"""
        correlations = []
        
        # ç°¡å–®çš„é—œè¯ï¼šåŸºæ–¼é—œéµè©åŒ¹é…
        text_keywords = workflow_text.lower().split()
        
        for event in events:
            if event.text_content:
                event_keywords = event.text_content.lower().split()
                common_keywords = set(text_keywords) & set(event_keywords)
                
                if common_keywords:
                    correlation = {
                        "event_id": event.event_id,
                        "timestamp": event.timestamp,
                        "common_keywords": list(common_keywords),
                        "correlation_score": len(common_keywords) / max(len(text_keywords), len(event_keywords))
                    }
                    correlations.append(correlation)
        
        return correlations
    
    def _calculate_video_quality(self, frames: List[VideoFrame], events: List[VideoEvent]) -> float:
        """è¨ˆç®—è¦–é »è³ªé‡è©•åˆ†"""
        quality_score = 0.0
        
        # åŸºæ–¼å¹€æ•¸é‡è©•åˆ† (30%)
        frame_score = min(len(frames) / 100, 1.0)  # 100å¹€ç‚ºæ»¿åˆ†
        quality_score += frame_score * 0.3
        
        # åŸºæ–¼äº‹ä»¶æª¢æ¸¬è©•åˆ† (40%)
        event_score = min(len(events) / 20, 1.0)  # 20å€‹äº‹ä»¶ç‚ºæ»¿åˆ†
        quality_score += event_score * 0.4
        
        # åŸºæ–¼éŒ¯èª¤ç‡è©•åˆ† (30%)
        error_events = [e for e in events if e.event_type == VideoEventType.ERROR]
        error_rate = len(error_events) / len(events) if events else 0
        error_score = max(0, 1.0 - error_rate * 2)  # éŒ¯èª¤ç‡è¶Šä½åˆ†æ•¸è¶Šé«˜
        quality_score += error_score * 0.3
        
        return min(quality_score, 1.0)

class UIElementDetector:
    """UIå…ƒç´ æª¢æ¸¬å™¨"""
    
    def __init__(self):
        self.element_templates = self._load_element_templates()
    
    def _load_element_templates(self) -> Dict[str, Any]:
        """åŠ è¼‰UIå…ƒç´ æ¨¡æ¿"""
        return {
            "button": {"min_area": 500, "aspect_ratio_range": (0.2, 5.0)},
            "input_field": {"min_area": 1000, "aspect_ratio_range": (2.0, 20.0)},
            "dropdown": {"min_area": 800, "aspect_ratio_range": (1.0, 10.0)},
            "checkbox": {"min_area": 100, "aspect_ratio_range": (0.8, 1.2)},
            "link": {"color_signature": (0, 0, 255), "underline_detection": True}
        }
    
    def detect_elements(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """æª¢æ¸¬UIå…ƒç´ """
        elements = []
        
        # é€™è£¡æ‡‰è©²å¯¦ç¾å¯¦éš›çš„UIå…ƒç´ æª¢æ¸¬ç®—æ³•
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šè¿”å›æ¨¡æ“¬æ•¸æ“š
        elements.append({
            "type": "button",
            "bbox": (100, 200, 80, 30),
            "confidence": 0.8,
            "text": "Submit"
        })
        
        return elements

class VideoTextExtractor:
    """è¦–é »æ–‡æœ¬æå–å™¨"""
    
    def __init__(self):
        # é€™è£¡å¯ä»¥åˆå§‹åŒ–OCRå¼•æ“ï¼Œå¦‚pytesseract
        pass
    
    def extract_text(self, frame: np.ndarray) -> str:
        """å¾å¹€ä¸­æå–æ–‡æœ¬"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šè¿”å›æ¨¡æ“¬æ–‡æœ¬
        # å¯¦éš›å¯¦ç¾æ‡‰è©²ä½¿ç”¨OCRæŠ€è¡“
        return f"Frame text at {time.time()}"

class TestCaseRefinementEngine:
    """æ¸¬è©¦ç”¨ä¾‹ç´°åŒ–å¼•æ“"""
    
    def __init__(self):
        self.refinement_strategies = self._load_refinement_strategies()
        self.pattern_analyzer = TestPatternAnalyzer()
    
    def _load_refinement_strategies(self) -> Dict[str, Any]:
        """åŠ è¼‰ç´°åŒ–ç­–ç•¥"""
        return {
            "event_based_refinement": self._refine_by_events,
            "error_based_refinement": self._refine_by_errors,
            "performance_based_refinement": self._refine_by_performance,
            "workflow_based_refinement": self._refine_by_workflow
        }
    
    def refine_test_cases(self, video_analysis: VideoAnalysisResult, 
                         original_test_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºæ–¼è¦–é »åˆ†æçµæœç´°åŒ–æ¸¬è©¦ç”¨ä¾‹"""
        logger.info(f"ğŸ”§ é–‹å§‹ç´°åŒ–æ¸¬è©¦ç”¨ä¾‹ï¼ŒåŸå§‹ç”¨ä¾‹æ•¸: {len(original_test_cases)}")
        
        refined_cases = []
        
        # åŸºæ–¼æª¢æ¸¬åˆ°çš„äº‹ä»¶ç´°åŒ–
        event_refined_cases = self._refine_by_events(video_analysis, original_test_cases)
        refined_cases.extend(event_refined_cases)
        
        # åŸºæ–¼éŒ¯èª¤ç‰‡æ®µç´°åŒ–
        error_refined_cases = self._refine_by_errors(video_analysis, original_test_cases)
        refined_cases.extend(error_refined_cases)
        
        # åŸºæ–¼æ€§èƒ½æŒ‡æ¨™ç´°åŒ–
        performance_refined_cases = self._refine_by_performance(video_analysis, original_test_cases)
        refined_cases.extend(performance_refined_cases)
        
        # åŸºæ–¼å·¥ä½œæµæ­¥é©Ÿç´°åŒ–
        workflow_refined_cases = self._refine_by_workflow(video_analysis, original_test_cases)
        refined_cases.extend(workflow_refined_cases)
        
        # å»é‡å’Œè³ªé‡è©•ä¼°
        unique_cases = self._deduplicate_test_cases(refined_cases)
        quality_scored_cases = self._score_test_cases(unique_cases, video_analysis)
        
        logger.info(f"âœ… æ¸¬è©¦ç”¨ä¾‹ç´°åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(quality_scored_cases)} å€‹å„ªåŒ–ç”¨ä¾‹")
        return quality_scored_cases
    
    def _refine_by_events(self, video_analysis: VideoAnalysisResult, 
                         original_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºæ–¼äº‹ä»¶ç´°åŒ–æ¸¬è©¦ç”¨ä¾‹"""
        refined_cases = []
        
        for event in video_analysis.detected_events:
            if event.event_type == VideoEventType.CLICK:
                # ç‚ºæ¯å€‹é»æ“Šäº‹ä»¶ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
                test_case = {
                    "id": f"click_test_{event.event_id}",
                    "type": TestCaseType.UI_INTERACTION.value,
                    "title": f"é»æ“Šæ¸¬è©¦ - åº§æ¨™({event.coordinates[0]}, {event.coordinates[1]})",
                    "description": f"æ¸¬è©¦åœ¨åº§æ¨™({event.coordinates[0]}, {event.coordinates[1]})çš„é»æ“Šæ“ä½œ",
                    "steps": [
                        f"1. å°èˆªåˆ°ç›®æ¨™é é¢",
                        f"2. é»æ“Šåº§æ¨™({event.coordinates[0]}, {event.coordinates[1]})",
                        f"3. é©—è­‰é»æ“ŠéŸ¿æ‡‰"
                    ],
                    "expected_result": "é»æ“Šæ“ä½œæˆåŠŸåŸ·è¡Œï¼Œç³»çµ±æ­£ç¢ºéŸ¿æ‡‰",
                    "video_timestamp": event.timestamp,
                    "confidence": event.confidence,
                    "source": "video_event_analysis",
                    "metadata": {
                        "event_type": event.event_type.value,
                        "coordinates": event.coordinates,
                        "duration": event.duration
                    }
                }
                refined_cases.append(test_case)
            
            elif event.event_type == VideoEventType.INPUT:
                # ç‚ºè¼¸å…¥äº‹ä»¶ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
                test_case = {
                    "id": f"input_test_{event.event_id}",
                    "type": TestCaseType.FUNCTIONAL.value,
                    "title": f"è¼¸å…¥æ¸¬è©¦ - {event.text_content[:20]}...",
                    "description": f"æ¸¬è©¦è¼¸å…¥å…§å®¹: {event.text_content}",
                    "steps": [
                        f"1. å®šä½è¼¸å…¥å­—æ®µ",
                        f"2. è¼¸å…¥å…§å®¹: {event.text_content}",
                        f"3. é©—è­‰è¼¸å…¥çµæœ"
                    ],
                    "expected_result": "è¼¸å…¥å…§å®¹æ­£ç¢ºé¡¯ç¤ºå’Œè™•ç†",
                    "video_timestamp": event.timestamp,
                    "confidence": event.confidence,
                    "source": "video_event_analysis",
                    "test_data": {"input_text": event.text_content},
                    "metadata": {
                        "event_type": event.event_type.value,
                        "text_content": event.text_content
                    }
                }
                refined_cases.append(test_case)
        
        return refined_cases
    
    def _refine_by_errors(self, video_analysis: VideoAnalysisResult, 
                         original_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºæ–¼éŒ¯èª¤ç´°åŒ–æ¸¬è©¦ç”¨ä¾‹"""
        refined_cases = []
        
        for error_segment in video_analysis.error_segments:
            test_case = {
                "id": f"error_test_{hash(error_segment['description'])}",
                "type": TestCaseType.ERROR_HANDLING.value,
                "title": f"éŒ¯èª¤è™•ç†æ¸¬è©¦ - {error_segment['description']}",
                "description": f"æ¸¬è©¦éŒ¯èª¤æƒ…æ³çš„è™•ç†: {error_segment['description']}",
                "steps": [
                    "1. é‡ç¾å°è‡´éŒ¯èª¤çš„æ“ä½œåºåˆ—",
                    "2. è§€å¯ŸéŒ¯èª¤çš„é¡¯ç¤ºå’Œè™•ç†",
                    "3. é©—è­‰éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶"
                ],
                "expected_result": "ç³»çµ±æ­£ç¢ºé¡¯ç¤ºéŒ¯èª¤ä¿¡æ¯ä¸¦æä¾›æ¢å¾©é¸é …",
                "video_timestamp": error_segment['start_time'],
                "confidence": error_segment['confidence'],
                "source": "video_error_analysis",
                "priority": "high",
                "metadata": {
                    "error_type": error_segment['error_type'],
                    "error_duration": error_segment['end_time'] - error_segment['start_time']
                }
            }
            refined_cases.append(test_case)
        
        return refined_cases
    
    def _refine_by_performance(self, video_analysis: VideoAnalysisResult, 
                              original_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºæ–¼æ€§èƒ½æŒ‡æ¨™ç´°åŒ–æ¸¬è©¦ç”¨ä¾‹"""
        refined_cases = []
        
        metrics = video_analysis.performance_metrics
        
        # å¦‚æœäº‹ä»¶å¯†åº¦éé«˜ï¼Œç”Ÿæˆæ€§èƒ½æ¸¬è©¦ç”¨ä¾‹
        if metrics.get('events_per_second', 0) > 2:
            test_case = {
                "id": "performance_high_activity",
                "type": TestCaseType.PERFORMANCE.value,
                "title": "é«˜æ´»å‹•å¯†åº¦æ€§èƒ½æ¸¬è©¦",
                "description": f"æ¸¬è©¦é«˜æ´»å‹•å¯†åº¦ä¸‹çš„ç³»çµ±æ€§èƒ½ ({metrics['events_per_second']:.2f} äº‹ä»¶/ç§’)",
                "steps": [
                    "1. åŸ·è¡Œé«˜é »ç‡æ“ä½œåºåˆ—",
                    "2. ç›£æ§ç³»çµ±éŸ¿æ‡‰æ™‚é–“",
                    "3. é©—è­‰ç³»çµ±ç©©å®šæ€§"
                ],
                "expected_result": "ç³»çµ±åœ¨é«˜æ´»å‹•å¯†åº¦ä¸‹ä¿æŒç©©å®šæ€§èƒ½",
                "confidence": 0.7,
                "source": "video_performance_analysis",
                "metadata": {
                    "events_per_second": metrics['events_per_second'],
                    "total_events": metrics['total_events']
                }
            }
            refined_cases.append(test_case)
        
        # å¦‚æœå¹³å‡äº‹ä»¶æŒçºŒæ™‚é–“éé•·ï¼Œç”ŸæˆéŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
        if metrics.get('average_event_duration', 0) > 2:
            test_case = {
                "id": "performance_response_time",
                "type": TestCaseType.PERFORMANCE.value,
                "title": "éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦",
                "description": f"æ¸¬è©¦ç³»çµ±éŸ¿æ‡‰æ™‚é–“ (å¹³å‡: {metrics['average_event_duration']:.2f}ç§’)",
                "steps": [
                    "1. åŸ·è¡Œæ¨™æº–æ“ä½œåºåˆ—",
                    "2. æ¸¬é‡æ¯å€‹æ“ä½œçš„éŸ¿æ‡‰æ™‚é–“",
                    "3. é©—è­‰éŸ¿æ‡‰æ™‚é–“åœ¨å¯æ¥å—ç¯„åœå…§"
                ],
                "expected_result": "æ‰€æœ‰æ“ä½œéŸ¿æ‡‰æ™‚é–“åœ¨2ç§’å…§",
                "confidence": 0.8,
                "source": "video_performance_analysis",
                "metadata": {
                    "average_duration": metrics['average_event_duration']
                }
            }
            refined_cases.append(test_case)
        
        return refined_cases
    
    def _refine_by_workflow(self, video_analysis: VideoAnalysisResult, 
                           original_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºæ–¼å·¥ä½œæµæ­¥é©Ÿç´°åŒ–æ¸¬è©¦ç”¨ä¾‹"""
        refined_cases = []
        
        for step in video_analysis.workflow_steps:
            if step.video_events:
                test_case = {
                    "id": f"workflow_step_{step.step_id}",
                    "type": TestCaseType.FUNCTIONAL.value,
                    "title": f"å·¥ä½œæµæ­¥é©Ÿæ¸¬è©¦ - {step.description[:30]}...",
                    "description": f"æ¸¬è©¦å·¥ä½œæµæ­¥é©Ÿ: {step.description}",
                    "steps": [
                        f"1. æº–å‚™æ­¥é©Ÿå‰ç½®æ¢ä»¶",
                        f"2. åŸ·è¡Œæ­¥é©Ÿ: {step.description}",
                        f"3. é©—è­‰æ­¥é©Ÿçµæœ: {step.expected_result}"
                    ],
                    "expected_result": step.expected_result,
                    "video_timestamp": step.start_time,
                    "confidence": 0.8,
                    "source": "video_workflow_analysis",
                    "metadata": {
                        "step_number": step.step_number,
                        "step_duration": (step.end_time - step.start_time) if step.end_time and step.start_time else 0,
                        "event_count": len(step.video_events)
                    }
                }
                refined_cases.append(test_case)
        
        return refined_cases
    
    def _deduplicate_test_cases(self, test_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ¸¬è©¦ç”¨ä¾‹"""
        unique_cases = []
        seen_descriptions = set()
        
        for case in test_cases:
            description_hash = hashlib.md5(case['description'].encode()).hexdigest()
            if description_hash not in seen_descriptions:
                seen_descriptions.add(description_hash)
                unique_cases.append(case)
        
        logger.info(f"ğŸ”„ å»é‡å¾Œä¿ç•™ {len(unique_cases)} å€‹å”¯ä¸€æ¸¬è©¦ç”¨ä¾‹")
        return unique_cases
    
    def _score_test_cases(self, test_cases: List[Dict[str, Any]], 
                         video_analysis: VideoAnalysisResult) -> List[Dict[str, Any]]:
        """ç‚ºæ¸¬è©¦ç”¨ä¾‹è©•åˆ†"""
        for case in test_cases:
            score = 0.0
            
            # åŸºæ–¼ä¿¡å¿ƒåº¦è©•åˆ† (40%)
            confidence = case.get('confidence', 0.5)
            score += confidence * 0.4
            
            # åŸºæ–¼è¦–é »è³ªé‡è©•åˆ† (30%)
            score += video_analysis.quality_score * 0.3
            
            # åŸºæ–¼æ¸¬è©¦é¡å‹é‡è¦æ€§è©•åˆ† (30%)
            type_weights = {
                TestCaseType.ERROR_HANDLING.value: 1.0,
                TestCaseType.FUNCTIONAL.value: 0.8,
                TestCaseType.UI_INTERACTION.value: 0.7,
                TestCaseType.PERFORMANCE.value: 0.6,
                TestCaseType.REGRESSION.value: 0.5
            }
            type_weight = type_weights.get(case.get('type', ''), 0.5)
            score += type_weight * 0.3
            
            case['quality_score'] = min(score, 1.0)
        
        # æŒ‰è³ªé‡è©•åˆ†æ’åº
        test_cases.sort(key=lambda x: x['quality_score'], reverse=True)
        
        return test_cases

class TestPatternAnalyzer:
    """æ¸¬è©¦æ¨¡å¼åˆ†æå™¨"""
    
    def __init__(self):
        self.learned_patterns = {}
    
    def analyze_patterns(self, video_analysis: VideoAnalysisResult) -> Dict[str, Any]:
        """åˆ†ææ¸¬è©¦æ¨¡å¼"""
        patterns = {
            "interaction_patterns": self._analyze_interaction_patterns(video_analysis.detected_events),
            "error_patterns": self._analyze_error_patterns(video_analysis.error_segments),
            "timing_patterns": self._analyze_timing_patterns(video_analysis.detected_events),
            "workflow_patterns": self._analyze_workflow_patterns(video_analysis.workflow_steps)
        }
        
        return patterns
    
    def _analyze_interaction_patterns(self, events: List[VideoEvent]) -> Dict[str, Any]:
        """åˆ†æäº¤äº’æ¨¡å¼"""
        click_events = [e for e in events if e.event_type == VideoEventType.CLICK]
        input_events = [e for e in events if e.event_type == VideoEventType.INPUT]
        
        return {
            "click_frequency": len(click_events),
            "input_frequency": len(input_events),
            "click_input_ratio": len(click_events) / max(len(input_events), 1),
            "common_coordinates": self._find_common_coordinates(click_events)
        }
    
    def _analyze_error_patterns(self, error_segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æéŒ¯èª¤æ¨¡å¼"""
        return {
            "error_frequency": len(error_segments),
            "average_error_duration": np.mean([seg['end_time'] - seg['start_time'] for seg in error_segments]) if error_segments else 0,
            "error_types": [seg['error_type'] for seg in error_segments]
        }
    
    def _analyze_timing_patterns(self, events: List[VideoEvent]) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        if not events:
            return {}
        
        intervals = []
        for i in range(1, len(events)):
            interval = events[i].timestamp - events[i-1].timestamp
            intervals.append(interval)
        
        return {
            "average_interval": np.mean(intervals) if intervals else 0,
            "min_interval": np.min(intervals) if intervals else 0,
            "max_interval": np.max(intervals) if intervals else 0,
            "interval_variance": np.var(intervals) if intervals else 0
        }
    
    def _analyze_workflow_patterns(self, workflow_steps: List[WorkflowStep]) -> Dict[str, Any]:
        """åˆ†æå·¥ä½œæµæ¨¡å¼"""
        return {
            "total_steps": len(workflow_steps),
            "completed_steps": len([s for s in workflow_steps if s.status == "completed"]),
            "average_step_duration": np.mean([
                (s.end_time - s.start_time) for s in workflow_steps 
                if s.start_time and s.end_time
            ]) if workflow_steps else 0
        }
    
    def _find_common_coordinates(self, click_events: List[VideoEvent]) -> List[Tuple[int, int]]:
        """æ‰¾å‡ºå¸¸è¦‹çš„é»æ“Šåº§æ¨™"""
        coordinates = [e.coordinates for e in click_events if e.coordinates]
        
        # ç°¡å–®çš„èšé¡ï¼šæ‰¾å‡ºè·é›¢è¼ƒè¿‘çš„åº§æ¨™
        common_coords = []
        threshold = 50  # åƒç´ è·é›¢é–¾å€¼
        
        for coord in coordinates:
            is_common = False
            for common_coord in common_coords:
                distance = np.sqrt((coord[0] - common_coord[0])**2 + (coord[1] - common_coord[1])**2)
                if distance < threshold:
                    is_common = True
                    break
            
            if not is_common:
                common_coords.append(coord)
        
        return common_coords

# æ¸¬è©¦å‡½æ•¸
def test_video_driven_optimization():
    """æ¸¬è©¦è¦–é »é©…å‹•æ¸¬è©¦ç”¨ä¾‹å„ªåŒ–ç³»çµ±"""
    print("ğŸ¬ æ¸¬è©¦è¦–é »é©…å‹•æ¸¬è©¦ç”¨ä¾‹å„ªåŒ–ç³»çµ±")
    print("=" * 80)
    
    # å‰µå»ºæ¨¡æ“¬è¦–é »åˆ†æçµæœ
    mock_events = [
        VideoEvent(
            event_id="click_001",
            event_type=VideoEventType.CLICK,
            timestamp=1.5,
            duration=0.2,
            coordinates=(300, 200),
            confidence=0.8
        ),
        VideoEvent(
            event_id="input_001",
            event_type=VideoEventType.INPUT,
            timestamp=3.0,
            duration=2.0,
            text_content="test@example.com",
            confidence=0.9
        ),
        VideoEvent(
            event_id="error_001",
            event_type=VideoEventType.ERROR,
            timestamp=6.0,
            duration=1.0,
            text_content="Invalid password",
            confidence=0.85
        )
    ]
    
    mock_workflow_steps = [
        WorkflowStep(
            step_id="step_001",
            step_number=1,
            description="è¼¸å…¥ç”¨æˆ¶å",
            expected_result="ç”¨æˆ¶åæ­£ç¢ºè¼¸å…¥",
            video_events=[mock_events[1]],
            start_time=2.5,
            end_time=5.0,
            status="completed"
        )
    ]
    
    mock_analysis = VideoAnalysisResult(
        video_id="test_video_001",
        video_path="/path/to/test_video.mp4",
        analysis_timestamp=datetime.now(),
        total_duration=10.0,
        frame_count=300,
        fps=30.0,
        resolution=(1920, 1080),
        quality_score=0.75,
        detected_events=mock_events,
        workflow_steps=mock_workflow_steps,
        error_segments=[{
            "start_time": 6.0,
            "end_time": 7.0,
            "error_type": "authentication_error",
            "description": "Invalid password",
            "confidence": 0.85
        }],
        performance_metrics={
            "total_events": 3,
            "events_per_second": 0.3,
            "average_event_duration": 1.07,
            "error_rate": 0.33
        },
        text_video_correlations=[]
    )
    
    # å‰µå»ºåŸå§‹æ¸¬è©¦ç”¨ä¾‹
    original_test_cases = [
        {
            "id": "original_001",
            "type": "functional",
            "title": "ç”¨æˆ¶ç™»éŒ„æ¸¬è©¦",
            "description": "æ¸¬è©¦ç”¨æˆ¶ç™»éŒ„åŠŸèƒ½",
            "steps": ["1. è¼¸å…¥ç”¨æˆ¶å", "2. è¼¸å…¥å¯†ç¢¼", "3. é»æ“Šç™»éŒ„"],
            "expected_result": "æˆåŠŸç™»éŒ„"
        }
    ]
    
    # æ¸¬è©¦ç´°åŒ–å¼•æ“
    refinement_engine = TestCaseRefinementEngine()
    
    print("\nğŸ”§ é–‹å§‹æ¸¬è©¦ç”¨ä¾‹ç´°åŒ–")
    print("-" * 60)
    
    refined_cases = refinement_engine.refine_test_cases(mock_analysis, original_test_cases)
    
    print(f"âœ… æ¸¬è©¦ç”¨ä¾‹ç´°åŒ–å®Œæˆ")
    print(f"ğŸ“Š åŸå§‹æ¸¬è©¦ç”¨ä¾‹æ•¸: {len(original_test_cases)}")
    print(f"ğŸ”§ ç´°åŒ–å¾Œæ¸¬è©¦ç”¨ä¾‹æ•¸: {len(refined_cases)}")
    
    # é¡¯ç¤ºç´°åŒ–çµæœ
    print(f"\nğŸ“‹ ç´°åŒ–å¾Œçš„æ¸¬è©¦ç”¨ä¾‹:")
    print("-" * 60)
    
    for i, case in enumerate(refined_cases[:5]):  # åªé¡¯ç¤ºå‰5å€‹
        print(f"{i+1}. {case['title']}")
        print(f"   é¡å‹: {case['type']}")
        print(f"   è³ªé‡è©•åˆ†: {case.get('quality_score', 0):.2f}")
        print(f"   ä¾†æº: {case.get('source', 'unknown')}")
        if 'video_timestamp' in case:
            print(f"   è¦–é »æ™‚é–“æˆ³: {case['video_timestamp']:.2f}s")
        print()
    
    # æ¸¬è©¦æ¨¡å¼åˆ†æ
    pattern_analyzer = TestPatternAnalyzer()
    patterns = pattern_analyzer.analyze_patterns(mock_analysis)
    
    print(f"ğŸ“Š æ¨¡å¼åˆ†æçµæœ:")
    print("-" * 60)
    print(f"äº¤äº’æ¨¡å¼: {patterns['interaction_patterns']}")
    print(f"éŒ¯èª¤æ¨¡å¼: {patterns['error_patterns']}")
    print(f"æ™‚é–“æ¨¡å¼: {patterns['timing_patterns']}")
    
    print("\nğŸ‰ è¦–é »é©…å‹•æ¸¬è©¦ç”¨ä¾‹å„ªåŒ–ç³»çµ±æ¸¬è©¦å®Œæˆï¼")
    
    return {
        "original_cases": original_test_cases,
        "refined_cases": refined_cases,
        "analysis_result": mock_analysis,
        "patterns": patterns
    }

if __name__ == "__main__":
    test_video_driven_optimization()

