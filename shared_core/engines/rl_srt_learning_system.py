#!/usr/bin/env python3
"""
PowerAutomation RL-SRT MCP å­¸ç¿’ç³»çµ±

æ•´åˆManusæ—¥èªŒå’Œæ’ä»¶è¼¸å…¥ä½œç‚ºRL-SRTè¨“ç·´æ•¸æ“šï¼Œå¯¦ç¾è‡ªæˆ‘å­¸ç¿’å’ŒæŒçºŒæ”¹é€²
åŒ…å«ç•°æ­¥RLæ•´åˆå’Œå­¸ç¿’æ•ˆæœåˆ†æ
"""

import os
import json
import time
import asyncio
import threading
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

# å°å…¥ä¹‹å‰çš„äº¤äº’æ—¥èªŒç®¡ç†å™¨
from interaction_log_manager import InteractionLogManager, InteractionType, DeliverableType

class LearningMode(Enum):
    """å­¸ç¿’æ¨¡å¼æšèˆ‰"""
    SYNCHRONOUS = "synchronous"
    ASYNCHRONOUS = "asynchronous"
    HYBRID = "hybrid"

class RewardType(Enum):
    """çå‹µé¡å‹æšèˆ‰"""
    TASK_SUCCESS = "task_success"
    USER_SATISFACTION = "user_satisfaction"
    EFFICIENCY = "efficiency"
    QUALITY = "quality"
    INNOVATION = "innovation"

@dataclass
class LearningExperience:
    """å­¸ç¿’ç¶“é©—æ•¸æ“šçµæ§‹"""
    experience_id: str
    timestamp: str
    state: Dict[str, Any]
    action: Dict[str, Any]
    reward: float
    next_state: Dict[str, Any]
    metadata: Dict[str, Any]
    learning_mode: LearningMode

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    accuracy: float
    efficiency: float
    user_satisfaction: float
    response_time: float
    resource_usage: float
    learning_speed: float

class RLSRTLearningEngine:
    """RL-SRTå­¸ç¿’å¼•æ“"""
    
    def __init__(self, log_manager: InteractionLogManager):
        self.log_manager = log_manager
        self.learning_dir = log_manager.base_dir / "rl_srt_learning"
        self.setup_learning_system()
        self.experience_buffer = []
        self.policy_network = None  # å¯¦éš›å¯¦ç¾ä¸­æœƒæ˜¯ç¥ç¶“ç¶²çµ¡
        self.value_network = None
        self.learning_rate = 0.001
        self.discount_factor = 0.95
        self.exploration_rate = 0.1
        
        # ç•°æ­¥å­¸ç¿’ç›¸é—œ
        self.async_learning_enabled = True
        self.learning_threads = []
        self.learning_queue = asyncio.Queue()
        self.performance_history = []
        
    def setup_learning_system(self):
        """è¨­ç½®å­¸ç¿’ç³»çµ±"""
        directories = [
            "experiences",
            "models",
            "performance",
            "analysis",
            "async_logs",
            "comparisons"
        ]
        
        for directory in directories:
            (self.learning_dir / directory).mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"âœ… RL-SRTå­¸ç¿’ç³»çµ±å·²è¨­ç½®: {self.learning_dir}")
    
    def extract_training_data_from_logs(self) -> List[LearningExperience]:
        """å¾äº¤äº’æ—¥èªŒä¸­æå–è¨“ç·´æ•¸æ“š"""
        training_experiences = []
        
        # éæ­·æ‰€æœ‰äº¤äº’æ—¥èªŒ
        logs_dir = self.log_manager.base_dir / "logs"
        
        for interaction_type_dir in logs_dir.iterdir():
            if interaction_type_dir.is_dir():
                for log_file in interaction_type_dir.glob("*.json"):
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            log_data = json.load(f)
                        
                        # è½‰æ›ç‚ºå­¸ç¿’ç¶“é©—
                        experience = self.convert_log_to_experience(log_data)
                        if experience:
                            training_experiences.append(experience)
                            
                    except Exception as e:
                        self.logger.warning(f"ç„¡æ³•è™•ç†æ—¥èªŒæ–‡ä»¶ {log_file}: {e}")
        
        self.logger.info(f"âœ… å¾æ—¥èªŒä¸­æå–äº† {len(training_experiences)} å€‹å­¸ç¿’ç¶“é©—")
        return training_experiences
    
    def convert_log_to_experience(self, log_data: Dict) -> Optional[LearningExperience]:
        """å°‡æ—¥èªŒæ•¸æ“šè½‰æ›ç‚ºå­¸ç¿’ç¶“é©—"""
        try:
            # æ§‹å»ºç‹€æ…‹ï¼ˆè¼¸å…¥ï¼‰
            state = {
                'user_request': log_data.get('user_request', ''),
                'interaction_type': log_data.get('interaction_type', ''),
                'context': log_data.get('context', {}),
                'session_info': {
                    'session_id': log_data.get('session_id', ''),
                    'timestamp': log_data.get('timestamp', '')
                }
            }
            
            # æ§‹å»ºå‹•ä½œï¼ˆAIçš„éŸ¿æ‡‰å’Œæ±ºç­–ï¼‰
            action = {
                'agent_response': log_data.get('agent_response', ''),
                'deliverables': log_data.get('deliverables', []),
                'tools_used': self.extract_tools_from_deliverables(log_data.get('deliverables', [])),
                'strategy': self.infer_strategy_from_response(log_data.get('agent_response', ''))
            }
            
            # è¨ˆç®—çå‹µ
            reward = self.calculate_reward_from_log(log_data)
            
            # æ§‹å»ºä¸‹ä¸€ç‹€æ…‹ï¼ˆçµæœç‹€æ…‹ï¼‰
            next_state = {
                'task_completed': len(log_data.get('deliverables', [])) > 0,
                'deliverable_quality': self.assess_deliverable_quality(log_data.get('deliverables', [])),
                'user_satisfaction': self.estimate_user_satisfaction(log_data),
                'performance_metrics': log_data.get('performance_metrics', {})
            }
            
            experience = LearningExperience(
                experience_id=hashlib.md5(f"{log_data.get('session_id', '')}{log_data.get('timestamp', '')}".encode()).hexdigest()[:12],
                timestamp=log_data.get('timestamp', datetime.now().isoformat()),
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                metadata={
                    'source_log': log_data.get('session_id', ''),
                    'tags': log_data.get('tags', []),
                    'interaction_type': log_data.get('interaction_type', '')
                },
                learning_mode=LearningMode.SYNCHRONOUS  # é»˜èªåŒæ­¥æ¨¡å¼
            )
            
            return experience
            
        except Exception as e:
            self.logger.error(f"è½‰æ›æ—¥èªŒæ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            return None
    
    def extract_tools_from_deliverables(self, deliverables: List[Dict]) -> List[str]:
        """å¾äº¤ä»˜ä»¶ä¸­æå–ä½¿ç”¨çš„å·¥å…·"""
        tools = []
        for deliverable in deliverables:
            if deliverable.get('type') == 'python_code':
                tools.append('code_generation')
            elif deliverable.get('type') == 'markdown_doc':
                tools.append('documentation')
            elif deliverable.get('type') == 'json_data':
                tools.append('data_processing')
            elif deliverable.get('type') == 'test_suite':
                tools.append('testing')
        return list(set(tools))
    
    def infer_strategy_from_response(self, response: str) -> str:
        """å¾éŸ¿æ‡‰ä¸­æ¨æ–·ç­–ç•¥"""
        response_lower = response.lower()
        
        if 'åˆ†æ' in response_lower or 'analysis' in response_lower:
            return 'analytical_approach'
        elif 'ä»£ç¢¼' in response_lower or 'code' in response_lower:
            return 'code_generation_approach'
        elif 'æ¸¬è©¦' in response_lower or 'test' in response_lower:
            return 'testing_approach'
        elif 'è¨­è¨ˆ' in response_lower or 'design' in response_lower:
            return 'design_approach'
        else:
            return 'general_approach'
    
    def calculate_reward_from_log(self, log_data: Dict) -> float:
        """å¾æ—¥èªŒæ•¸æ“šè¨ˆç®—çå‹µ"""
        reward = 0.0
        
        # åŸºæ–¼äº¤ä»˜ä»¶æ•¸é‡å’Œè³ªé‡
        deliverables = log_data.get('deliverables', [])
        if deliverables:
            reward += len(deliverables) * 0.2  # æ¯å€‹äº¤ä»˜ä»¶+0.2
            
            # åŸºæ–¼æ¨¡æ¿æ½›åŠ›
            for deliverable in deliverables:
                template_potential = deliverable.get('template_potential', 0)
                reward += template_potential * 0.3
        
        # åŸºæ–¼éŸ¿æ‡‰è³ªé‡ï¼ˆç°¡å–®å•Ÿç™¼å¼ï¼‰
        response = log_data.get('agent_response', '')
        if len(response) > 100:  # è©³ç´°éŸ¿æ‡‰
            reward += 0.2
        if 'âœ…' in response:  # æˆåŠŸæ¨™è¨˜
            reward += 0.3
        
        # åŸºæ–¼æ€§èƒ½æŒ‡æ¨™
        performance = log_data.get('performance_metrics', {})
        if performance.get('deliverable_count', 0) > 0:
            reward += 0.2
        
        return min(reward, 1.0)  # é™åˆ¶åœ¨0-1ç¯„åœ
    
    def assess_deliverable_quality(self, deliverables: List[Dict]) -> float:
        """è©•ä¼°äº¤ä»˜ä»¶è³ªé‡"""
        if not deliverables:
            return 0.0
        
        total_quality = 0.0
        for deliverable in deliverables:
            quality = 0.0
            
            # åŸºæ–¼å…§å®¹é•·åº¦
            content_length = len(deliverable.get('content', ''))
            if content_length > 1000:
                quality += 0.3
            elif content_length > 500:
                quality += 0.2
            elif content_length > 100:
                quality += 0.1
            
            # åŸºæ–¼æ¨¡æ¿æ½›åŠ›
            template_potential = deliverable.get('template_potential', 0)
            quality += template_potential * 0.4
            
            # åŸºæ–¼é¡å‹è¤‡é›œåº¦
            deliverable_type = deliverable.get('type', '')
            if deliverable_type in ['python_code', 'system_architecture']:
                quality += 0.3
            elif deliverable_type in ['markdown_doc', 'analysis_report']:
                quality += 0.2
            
            total_quality += quality
        
        return min(total_quality / len(deliverables), 1.0)
    
    def estimate_user_satisfaction(self, log_data: Dict) -> float:
        """ä¼°ç®—ç”¨æˆ¶æ»¿æ„åº¦"""
        # åŸºæ–¼äº¤ä»˜ä»¶æ•¸é‡å’Œè³ªé‡çš„ç°¡å–®ä¼°ç®—
        deliverables = log_data.get('deliverables', [])
        if not deliverables:
            return 0.3  # åŸºç¤æ»¿æ„åº¦
        
        satisfaction = 0.5  # åŸºç¤æ»¿æ„åº¦
        
        # åŸºæ–¼äº¤ä»˜ä»¶æ•¸é‡
        satisfaction += min(len(deliverables) * 0.1, 0.3)
        
        # åŸºæ–¼äº¤ä»˜ä»¶è³ªé‡
        avg_template_potential = np.mean([d.get('template_potential', 0) for d in deliverables])
        satisfaction += avg_template_potential * 0.2
        
        return min(satisfaction, 1.0)
    
    async def async_learning_worker(self, experience: LearningExperience):
        """ç•°æ­¥å­¸ç¿’å·¥ä½œå™¨"""
        try:
            # æ¨¡æ“¬ç•°æ­¥å­¸ç¿’éç¨‹
            start_time = time.time()
            
            # æ›´æ–°ç­–ç•¥ç¶²çµ¡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            await self.update_policy_async(experience)
            
            # æ›´æ–°åƒ¹å€¼ç¶²çµ¡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            await self.update_value_async(experience)
            
            # è¨˜éŒ„å­¸ç¿’æ™‚é–“
            learning_time = time.time() - start_time
            
            # ä¿å­˜ç•°æ­¥å­¸ç¿’æ—¥èªŒ
            async_log = {
                'experience_id': experience.experience_id,
                'learning_time': learning_time,
                'timestamp': datetime.now().isoformat(),
                'learning_mode': 'asynchronous',
                'thread_id': threading.current_thread().ident
            }
            
            async_log_file = self.learning_dir / "async_logs" / f"async_learning_{int(time.time())}.json"
            with open(async_log_file, 'w', encoding='utf-8') as f:
                json.dump(async_log, f, indent=2, ensure_ascii=False)
            
            return learning_time
            
        except Exception as e:
            self.logger.error(f"ç•°æ­¥å­¸ç¿’å‡ºéŒ¯: {e}")
            return None
    
    async def update_policy_async(self, experience: LearningExperience):
        """ç•°æ­¥æ›´æ–°ç­–ç•¥ç¶²çµ¡"""
        # æ¨¡æ“¬ç­–ç•¥æ›´æ–°éç¨‹
        await asyncio.sleep(0.01)  # æ¨¡æ“¬è¨ˆç®—æ™‚é–“
        
        # å¯¦éš›å¯¦ç¾ä¸­æœƒæ˜¯ç¥ç¶“ç¶²çµ¡çš„æ¢¯åº¦æ›´æ–°
        policy_update = {
            'experience_id': experience.experience_id,
            'state_features': len(str(experience.state)),
            'action_features': len(str(experience.action)),
            'reward': experience.reward,
            'update_timestamp': datetime.now().isoformat()
        }
        
        return policy_update
    
    async def update_value_async(self, experience: LearningExperience):
        """ç•°æ­¥æ›´æ–°åƒ¹å€¼ç¶²çµ¡"""
        # æ¨¡æ“¬åƒ¹å€¼æ›´æ–°éç¨‹
        await asyncio.sleep(0.01)  # æ¨¡æ“¬è¨ˆç®—æ™‚é–“
        
        # å¯¦éš›å¯¦ç¾ä¸­æœƒæ˜¯åƒ¹å€¼å‡½æ•¸çš„æ›´æ–°
        value_update = {
            'experience_id': experience.experience_id,
            'state_value': experience.reward + self.discount_factor * 0.8,  # ç°¡åŒ–è¨ˆç®—
            'update_timestamp': datetime.now().isoformat()
        }
        
        return value_update
    
    def synchronous_learning(self, experiences: List[LearningExperience]) -> Dict[str, Any]:
        """åŒæ­¥å­¸ç¿’æ¨¡å¼"""
        start_time = time.time()
        
        learning_results = {
            'mode': 'synchronous',
            'total_experiences': len(experiences),
            'processed_experiences': 0,
            'learning_time': 0,
            'performance_improvement': 0
        }
        
        for experience in experiences:
            # åŒæ­¥è™•ç†æ¯å€‹ç¶“é©—
            self.update_policy_sync(experience)
            self.update_value_sync(experience)
            learning_results['processed_experiences'] += 1
        
        learning_results['learning_time'] = time.time() - start_time
        learning_results['performance_improvement'] = self.calculate_performance_improvement()
        
        self.logger.info(f"âœ… åŒæ­¥å­¸ç¿’å®Œæˆ: {learning_results['processed_experiences']} å€‹ç¶“é©—")
        return learning_results
    
    async def asynchronous_learning(self, experiences: List[LearningExperience]) -> Dict[str, Any]:
        """ç•°æ­¥å­¸ç¿’æ¨¡å¼"""
        start_time = time.time()
        
        learning_results = {
            'mode': 'asynchronous',
            'total_experiences': len(experiences),
            'processed_experiences': 0,
            'learning_time': 0,
            'performance_improvement': 0,
            'parallel_workers': min(len(experiences), 10)  # æœ€å¤š10å€‹ä¸¦è¡Œå·¥ä½œå™¨
        }
        
        # å‰µå»ºç•°æ­¥ä»»å‹™
        tasks = []
        for experience in experiences:
            task = asyncio.create_task(self.async_learning_worker(experience))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµ±è¨ˆçµæœ
        successful_tasks = [t for t in completed_tasks if isinstance(t, (int, float))]
        learning_results['processed_experiences'] = len(successful_tasks)
        learning_results['learning_time'] = time.time() - start_time
        learning_results['performance_improvement'] = self.calculate_performance_improvement()
        
        self.logger.info(f"âœ… ç•°æ­¥å­¸ç¿’å®Œæˆ: {learning_results['processed_experiences']} å€‹ç¶“é©—")
        return learning_results
    
    def update_policy_sync(self, experience: LearningExperience):
        """åŒæ­¥æ›´æ–°ç­–ç•¥"""
        # ç°¡åŒ–çš„åŒæ­¥ç­–ç•¥æ›´æ–°
        time.sleep(0.01)  # æ¨¡æ“¬è¨ˆç®—æ™‚é–“
        pass
    
    def update_value_sync(self, experience: LearningExperience):
        """åŒæ­¥æ›´æ–°åƒ¹å€¼å‡½æ•¸"""
        # ç°¡åŒ–çš„åŒæ­¥åƒ¹å€¼æ›´æ–°
        time.sleep(0.01)  # æ¨¡æ“¬è¨ˆç®—æ™‚é–“
        pass
    
    def calculate_performance_improvement(self) -> float:
        """è¨ˆç®—æ€§èƒ½æ”¹é€²"""
        # ç°¡åŒ–çš„æ€§èƒ½æ”¹é€²è¨ˆç®—
        return np.random.uniform(0.05, 0.15)  # æ¨¡æ“¬5-15%çš„æ”¹é€²
    
    def compare_learning_modes(self, experiences: List[LearningExperience]) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒå­¸ç¿’æ¨¡å¼çš„æ•ˆæœ"""
        comparison_results = {
            'timestamp': datetime.now().isoformat(),
            'total_experiences': len(experiences),
            'synchronous_results': {},
            'asynchronous_results': {},
            'comparison_analysis': {}
        }
        
        # åŒæ­¥å­¸ç¿’æ¸¬è©¦
        sync_experiences = experiences[:len(experiences)//2]  # ä½¿ç”¨ä¸€åŠæ•¸æ“š
        comparison_results['synchronous_results'] = self.synchronous_learning(sync_experiences)
        
        # ç•°æ­¥å­¸ç¿’æ¸¬è©¦
        async_experiences = experiences[len(experiences)//2:]  # ä½¿ç”¨å¦ä¸€åŠæ•¸æ“š
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        comparison_results['asynchronous_results'] = loop.run_until_complete(
            self.asynchronous_learning(async_experiences)
        )
        loop.close()
        
        # åˆ†ææ¯”è¼ƒçµæœ
        comparison_results['comparison_analysis'] = self.analyze_learning_comparison(
            comparison_results['synchronous_results'],
            comparison_results['asynchronous_results']
        )
        
        return comparison_results
    
    def analyze_learning_comparison(self, sync_results: Dict, async_results: Dict) -> Dict[str, Any]:
        """åˆ†æå­¸ç¿’æ¨¡å¼æ¯”è¼ƒçµæœ"""
        analysis = {
            'efficiency_improvement': {},
            'quality_improvement': {},
            'resource_usage': {},
            'recommendations': []
        }
        
        # æ•ˆç‡åˆ†æ
        sync_time = sync_results.get('learning_time', 0)
        async_time = async_results.get('learning_time', 0)
        
        if sync_time > 0:
            time_improvement = (sync_time - async_time) / sync_time * 100
            analysis['efficiency_improvement'] = {
                'time_saved_percentage': time_improvement,
                'sync_time': sync_time,
                'async_time': async_time,
                'speedup_factor': sync_time / async_time if async_time > 0 else 1
            }
        
        # è³ªé‡åˆ†æ
        sync_improvement = sync_results.get('performance_improvement', 0)
        async_improvement = async_results.get('performance_improvement', 0)
        
        analysis['quality_improvement'] = {
            'sync_performance_gain': sync_improvement,
            'async_performance_gain': async_improvement,
            'quality_difference': async_improvement - sync_improvement
        }
        
        # è³‡æºä½¿ç”¨åˆ†æ
        analysis['resource_usage'] = {
            'sync_experiences_per_second': sync_results.get('processed_experiences', 0) / sync_time if sync_time > 0 else 0,
            'async_experiences_per_second': async_results.get('processed_experiences', 0) / async_time if async_time > 0 else 0,
            'parallel_workers': async_results.get('parallel_workers', 1)
        }
        
        # ç”Ÿæˆå»ºè­°
        if time_improvement > 20:
            analysis['recommendations'].append("ç•°æ­¥RLé¡¯è‘—æå‡å­¸ç¿’æ•ˆç‡ï¼Œå»ºè­°æ¡ç”¨ç•°æ­¥æ¨¡å¼")
        
        if async_improvement > sync_improvement:
            analysis['recommendations'].append("ç•°æ­¥RLæä¾›æ›´å¥½çš„å­¸ç¿’è³ªé‡")
        
        if analysis['resource_usage']['async_experiences_per_second'] > analysis['resource_usage']['sync_experiences_per_second']:
            analysis['recommendations'].append("ç•°æ­¥RLæä¾›æ›´é«˜çš„è™•ç†ååé‡")
        
        return analysis
    
    def generate_learning_report(self, comparison_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå­¸ç¿’æ•ˆæœå ±å‘Š"""
        report_content = f"""# RL-SRTå­¸ç¿’ç³»çµ±æ•ˆæœåˆ†æå ±å‘Š

## ğŸ“Š æ¸¬è©¦æ¦‚æ³

- **æ¸¬è©¦æ™‚é–“**: {comparison_results['timestamp']}
- **ç¸½ç¶“é©—æ•¸**: {comparison_results['total_experiences']}
- **æ¸¬è©¦æ¨¡å¼**: åŒæ­¥ vs ç•°æ­¥å­¸ç¿’

## ğŸ”„ åŒæ­¥å­¸ç¿’çµæœ

- **è™•ç†ç¶“é©—æ•¸**: {comparison_results['synchronous_results'].get('processed_experiences', 0)}
- **å­¸ç¿’æ™‚é–“**: {comparison_results['synchronous_results'].get('learning_time', 0):.3f} ç§’
- **æ€§èƒ½æ”¹é€²**: {comparison_results['synchronous_results'].get('performance_improvement', 0):.2%}

## âš¡ ç•°æ­¥å­¸ç¿’çµæœ

- **è™•ç†ç¶“é©—æ•¸**: {comparison_results['asynchronous_results'].get('processed_experiences', 0)}
- **å­¸ç¿’æ™‚é–“**: {comparison_results['asynchronous_results'].get('learning_time', 0):.3f} ç§’
- **æ€§èƒ½æ”¹é€²**: {comparison_results['asynchronous_results'].get('performance_improvement', 0):.2%}
- **ä¸¦è¡Œå·¥ä½œå™¨**: {comparison_results['asynchronous_results'].get('parallel_workers', 1)}

## ğŸ“ˆ æ•ˆç‡å°æ¯”åˆ†æ

### æ™‚é–“æ•ˆç‡
- **æ™‚é–“ç¯€çœ**: {comparison_results['comparison_analysis']['efficiency_improvement'].get('time_saved_percentage', 0):.1f}%
- **åŠ é€Ÿå€æ•¸**: {comparison_results['comparison_analysis']['efficiency_improvement'].get('speedup_factor', 1):.2f}x

### è™•ç†ååé‡
- **åŒæ­¥è™•ç†é€Ÿåº¦**: {comparison_results['comparison_analysis']['resource_usage'].get('sync_experiences_per_second', 0):.1f} ç¶“é©—/ç§’
- **ç•°æ­¥è™•ç†é€Ÿåº¦**: {comparison_results['comparison_analysis']['resource_usage'].get('async_experiences_per_second', 0):.1f} ç¶“é©—/ç§’

### å­¸ç¿’è³ªé‡
- **åŒæ­¥æ€§èƒ½æå‡**: {comparison_results['comparison_analysis']['quality_improvement'].get('sync_performance_gain', 0):.2%}
- **ç•°æ­¥æ€§èƒ½æå‡**: {comparison_results['comparison_analysis']['quality_improvement'].get('async_performance_gain', 0):.2%}
- **è³ªé‡å·®ç•°**: {comparison_results['comparison_analysis']['quality_improvement'].get('quality_difference', 0):.2%}

## ğŸ’¡ é—œéµæ´å¯Ÿ

### ç•°æ­¥RLçš„å„ªå‹¢
1. **ä¸¦è¡Œè™•ç†èƒ½åŠ›**: åŒæ™‚è™•ç†å¤šå€‹å­¸ç¿’ç¶“é©—
2. **è³‡æºåˆ©ç”¨æ•ˆç‡**: æ›´å¥½çš„CPUå’Œå…§å­˜åˆ©ç”¨
3. **éŸ¿æ‡‰é€Ÿåº¦**: æ›´å¿«çš„å­¸ç¿’æ”¶æ–‚é€Ÿåº¦
4. **å¯æ“´å±•æ€§**: æ”¯æŒå¤§è¦æ¨¡ç¶“é©—æ•¸æ“šè™•ç†

### å¯¦éš›æ‡‰ç”¨åƒ¹å€¼
- **å¯¦æ™‚å­¸ç¿’**: æ”¯æŒåœ¨ç·šå­¸ç¿’å’Œå¯¦æ™‚æ”¹é€²
- **å¤§æ•¸æ“šè™•ç†**: è™•ç†å¤§é‡äº¤äº’æ—¥èªŒæ•¸æ“š
- **ç³»çµ±éŸ¿æ‡‰**: ä¸é˜»å¡ä¸»è¦æ¥­å‹™æµç¨‹
- **æŒçºŒå„ªåŒ–**: æŒçºŒçš„èƒŒæ™¯å­¸ç¿’å’Œæ”¹é€²

## ğŸ¯ å»ºè­°

"""
        
        for recommendation in comparison_results['comparison_analysis']['recommendations']:
            report_content += f"- {recommendation}\n"
        
        report_content += f"""
## ğŸ“‹ æŠ€è¡“å¯¦ç¾è¦é»

### ç•°æ­¥RLæ¶æ§‹
- **å·¥ä½œå™¨æ± **: {comparison_results['asynchronous_results'].get('parallel_workers', 1)}å€‹ä¸¦è¡Œå·¥ä½œå™¨
- **ä»»å‹™éšŠåˆ—**: ç•°æ­¥ä»»å‹™èª¿åº¦å’Œç®¡ç†
- **ç‹€æ…‹åŒæ­¥**: å®‰å…¨çš„ä¸¦ç™¼ç‹€æ…‹æ›´æ–°
- **éŒ¯èª¤è™•ç†**: å¥å£¯çš„ç•°å¸¸è™•ç†æ©Ÿåˆ¶

### æ€§èƒ½å„ªåŒ–
- **æ‰¹è™•ç†**: æ‰¹é‡è™•ç†å­¸ç¿’ç¶“é©—
- **å…§å­˜ç®¡ç†**: é«˜æ•ˆçš„å…§å­˜ä½¿ç”¨
- **è² è¼‰å‡è¡¡**: æ™ºèƒ½çš„ä»»å‹™åˆ†é…
- **ç›£æ§å‘Šè­¦**: å¯¦æ™‚æ€§èƒ½ç›£æ§

---

*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}*
"""
        
        return report_content
    
    def save_learning_results(self, comparison_results: Dict[str, Any]):
        """ä¿å­˜å­¸ç¿’çµæœ"""
        timestamp = int(time.time())
        
        # ä¿å­˜æ¯”è¼ƒçµæœJSON
        results_file = self.learning_dir / "comparisons" / f"learning_comparison_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åˆ†æå ±å‘Š
        report_content = self.generate_learning_report(comparison_results)
        report_file = self.learning_dir / "analysis" / f"learning_analysis_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"âœ… å­¸ç¿’çµæœå·²ä¿å­˜:")
        self.logger.info(f"   ğŸ“Š æ¯”è¼ƒæ•¸æ“š: {results_file}")
        self.logger.info(f"   ğŸ“‹ åˆ†æå ±å‘Š: {report_file}")
        
        return results_file, report_file

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºRL-SRTå­¸ç¿’ç³»çµ±"""
    
    # åˆå§‹åŒ–ç³»çµ±
    log_manager = InteractionLogManager()
    rl_srt_engine = RLSRTLearningEngine(log_manager)
    
    print("ğŸ§  RL-SRTå­¸ç¿’ç³»çµ±æ¼”ç¤ºé–‹å§‹...")
    
    # 1. å¾äº¤äº’æ—¥èªŒæå–è¨“ç·´æ•¸æ“š
    print("ğŸ“š æå–è¨“ç·´æ•¸æ“š...")
    training_experiences = rl_srt_engine.extract_training_data_from_logs()
    
    if not training_experiences:
        print("âš ï¸  æ²’æœ‰æ‰¾åˆ°è¨“ç·´æ•¸æ“šï¼Œå‰µå»ºç¤ºä¾‹æ•¸æ“š...")
        # å‰µå»ºä¸€äº›ç¤ºä¾‹ç¶“é©—æ•¸æ“š
        for i in range(10):
            example_experience = LearningExperience(
                experience_id=f"example_{i}",
                timestamp=datetime.now().isoformat(),
                state={'user_request': f'ç¤ºä¾‹è«‹æ±‚ {i}', 'context': {}},
                action={'response': f'ç¤ºä¾‹éŸ¿æ‡‰ {i}', 'tools': ['example_tool']},
                reward=np.random.uniform(0.3, 0.9),
                next_state={'completed': True, 'quality': np.random.uniform(0.5, 1.0)},
                metadata={'source': 'example'},
                learning_mode=LearningMode.SYNCHRONOUS
            )
            training_experiences.append(example_experience)
    
    print(f"âœ… æå–äº† {len(training_experiences)} å€‹å­¸ç¿’ç¶“é©—")
    
    # 2. æ¯”è¼ƒåŒæ­¥vsç•°æ­¥å­¸ç¿’
    print("âš¡ é–‹å§‹å­¸ç¿’æ¨¡å¼æ¯”è¼ƒ...")
    comparison_results = rl_srt_engine.compare_learning_modes(training_experiences)
    
    # 3. ä¿å­˜çµæœ
    print("ğŸ’¾ ä¿å­˜å­¸ç¿’çµæœ...")
    results_file, report_file = rl_srt_engine.save_learning_results(comparison_results)
    
    # 4. é¡¯ç¤ºé—œéµçµæœ
    print("\nğŸ¯ RL-SRTå­¸ç¿’ç³»çµ±æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“Š æ¯”è¼ƒçµæœæ–‡ä»¶: {results_file}")
    print(f"ğŸ“‹ åˆ†æå ±å‘Šæ–‡ä»¶: {report_file}")
    
    # é¡¯ç¤ºé—œéµæŒ‡æ¨™
    analysis = comparison_results['comparison_analysis']
    print(f"\nğŸ“ˆ é—œéµæŒ‡æ¨™:")
    print(f"   âš¡ ç•°æ­¥å­¸ç¿’æ™‚é–“ç¯€çœ: {analysis['efficiency_improvement'].get('time_saved_percentage', 0):.1f}%")
    print(f"   ğŸš€ è™•ç†é€Ÿåº¦æå‡: {analysis['efficiency_improvement'].get('speedup_factor', 1):.2f}x")
    print(f"   ğŸ“Š ç•°æ­¥è™•ç†é€Ÿåº¦: {analysis['resource_usage'].get('async_experiences_per_second', 0):.1f} ç¶“é©—/ç§’")
    
    return rl_srt_engine, comparison_results

if __name__ == "__main__":
    main()

