#!/usr/bin/env python3
"""
PowerAutomation æ™ºèƒ½è°ƒåº¦ç®—æ³•å®ç°
åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å’ŒèŠ‚ç‚¹é€‰æ‹©

ä½œè€…: PowerAutomationå›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0-production
"""

import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
import joblib
import asyncio
import threading
from collections import defaultdict, deque

logger = logging.getLogger("PowerAutomation.SmartScheduler")

@dataclass
class NodePerformanceMetrics:
    """èŠ‚ç‚¹æ€§èƒ½æŒ‡æ ‡"""
    node_id: str
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_io: float
    network_io: float
    task_completion_rate: float
    average_execution_time: float
    error_rate: float
    concurrent_tasks: int
    
    def to_feature_vector(self) -> List[float]:
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        return [
            self.cpu_usage,
            self.memory_usage,
            self.disk_io,
            self.network_io,
            self.task_completion_rate,
            self.average_execution_time,
            self.error_rate,
            self.concurrent_tasks
        ]

@dataclass
class TaskCharacteristics:
    """ä»»åŠ¡ç‰¹å¾"""
    task_type: str
    test_level: str
    estimated_duration: float
    resource_requirements: Dict[str, Any]
    priority: int
    dependencies: List[str]
    
    def to_feature_vector(self) -> List[float]:
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        # ä»»åŠ¡ç±»å‹ç¼–ç 
        task_type_encoding = {
            "unit_test": 1, "integration_test": 2, "ui_test": 3,
            "performance_test": 4, "security_test": 5, "api_test": 6,
            "load_test": 7, "stress_test": 8, "end_to_end_test": 9
        }
        
        # æµ‹è¯•çº§åˆ«ç¼–ç 
        level_encoding = {f"level{i}": i for i in range(1, 11)}
        
        return [
            task_type_encoding.get(self.task_type, 0),
            level_encoding.get(self.test_level, 0),
            self.estimated_duration,
            self.resource_requirements.get("cpu_cores", 1),
            self.resource_requirements.get("memory_gb", 2),
            self.priority,
            len(self.dependencies)
        ]

class NodePerformancePredictor:
    """èŠ‚ç‚¹æ€§èƒ½é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_names = [
            'cpu_usage', 'memory_usage', 'disk_io', 'network_io',
            'task_completion_rate', 'avg_execution_time', 'error_rate', 'concurrent_tasks'
        ]
        
    def train(self, historical_data: List[NodePerformanceMetrics], target_metrics: List[float]):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        if len(historical_data) < 10:
            logger.warning("å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒé¢„æµ‹æ¨¡å‹")
            return False
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X = np.array([metrics.to_feature_vector() for metrics in historical_data])
            y = np.array(target_metrics)
            
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X_train, y_train)
            
            # è¯„ä¼°æ¨¡å‹
            y_pred = self.model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            
            logger.info(f"èŠ‚ç‚¹æ€§èƒ½é¢„æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒMSE: {mse:.4f}")
            self.is_trained = True
            return True
            
        except Exception as e:
            logger.error(f"èŠ‚ç‚¹æ€§èƒ½é¢„æµ‹æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def predict_performance(self, current_metrics: NodePerformanceMetrics) -> float:
        """é¢„æµ‹èŠ‚ç‚¹æ€§èƒ½"""
        if not self.is_trained:
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œè¿”å›åŸºäºå½“å‰æŒ‡æ ‡çš„ç®€å•è¯„åˆ†
            return self._simple_performance_score(current_metrics)
        
        try:
            X = np.array([current_metrics.to_feature_vector()])
            X_scaled = self.scaler.transform(X)
            prediction = self.model.predict(X_scaled)[0]
            return max(0, min(100, prediction))  # é™åˆ¶åœ¨0-100èŒƒå›´å†…
            
        except Exception as e:
            logger.error(f"æ€§èƒ½é¢„æµ‹å¤±è´¥: {e}")
            return self._simple_performance_score(current_metrics)
    
    def _simple_performance_score(self, metrics: NodePerformanceMetrics) -> float:
        """ç®€å•æ€§èƒ½è¯„åˆ†ç®—æ³•"""
        score = 100.0
        
        # CPUä½¿ç”¨ç‡å½±å“ (ä½¿ç”¨ç‡è¶Šä½è¶Šå¥½)
        score -= metrics.cpu_usage * 0.5
        
        # å†…å­˜ä½¿ç”¨ç‡å½±å“
        score -= metrics.memory_usage * 0.3
        
        # é”™è¯¯ç‡å½±å“
        score -= metrics.error_rate * 20
        
        # ä»»åŠ¡å®Œæˆç‡å½±å“ (å®Œæˆç‡è¶Šé«˜è¶Šå¥½)
        score += (metrics.task_completion_rate - 0.5) * 20
        
        # å¹¶å‘ä»»åŠ¡è´Ÿè½½å½±å“
        if metrics.concurrent_tasks > 5:
            score -= (metrics.concurrent_tasks - 5) * 5
        
        return max(0, min(100, score))

class TaskNodeMatcher:
    """ä»»åŠ¡èŠ‚ç‚¹åŒ¹é…å™¨"""
    
    def __init__(self):
        self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def train(self, training_data: List[Tuple[TaskCharacteristics, NodePerformanceMetrics, bool]]):
        """è®­ç»ƒåŒ¹é…æ¨¡å‹"""
        if len(training_data) < 20:
            logger.warning("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒåŒ¹é…æ¨¡å‹")
            return False
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X = []
            y = []
            
            for task_char, node_metrics, success in training_data:
                # ç»„åˆä»»åŠ¡ç‰¹å¾å’ŒèŠ‚ç‚¹ç‰¹å¾
                features = task_char.to_feature_vector() + node_metrics.to_feature_vector()
                X.append(features)
                y.append(1 if success else 0)
            
            X = np.array(X)
            y = np.array(y)
            
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X_train, y_train)
            
            # è¯„ä¼°æ¨¡å‹
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            logger.info(f"ä»»åŠ¡èŠ‚ç‚¹åŒ¹é…æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.4f}")
            self.is_trained = True
            return True
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡èŠ‚ç‚¹åŒ¹é…æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def predict_match_probability(self, task: TaskCharacteristics, node: NodePerformanceMetrics) -> float:
        """é¢„æµ‹ä»»åŠ¡èŠ‚ç‚¹åŒ¹é…æ¦‚ç‡"""
        if not self.is_trained:
            return self._simple_match_score(task, node)
        
        try:
            features = task.to_feature_vector() + node.to_feature_vector()
            X = np.array([features])
            X_scaled = self.scaler.transform(X)
            
            # è·å–åŒ¹é…æ¦‚ç‡
            probabilities = self.model.predict_proba(X_scaled)[0]
            return probabilities[1] if len(probabilities) > 1 else 0.5
            
        except Exception as e:
            logger.error(f"åŒ¹é…æ¦‚ç‡é¢„æµ‹å¤±è´¥: {e}")
            return self._simple_match_score(task, node)
    
    def _simple_match_score(self, task: TaskCharacteristics, node: NodePerformanceMetrics) -> float:
        """ç®€å•åŒ¹é…è¯„åˆ†ç®—æ³•"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # èµ„æºåŒ¹é…åº¦
        required_cpu = task.resource_requirements.get("cpu_cores", 1)
        required_memory = task.resource_requirements.get("memory_gb", 2)
        
        # CPUåŒ¹é…åº¦ (å‡è®¾èŠ‚ç‚¹æœ‰è¶³å¤ŸCPU)
        if node.cpu_usage < 70:  # CPUä½¿ç”¨ç‡ä½äº70%
            score += 0.2
        elif node.cpu_usage > 90:
            score -= 0.3
        
        # å†…å­˜åŒ¹é…åº¦
        if node.memory_usage < 60:  # å†…å­˜ä½¿ç”¨ç‡ä½äº60%
            score += 0.2
        elif node.memory_usage > 85:
            score -= 0.3
        
        # å¹¶å‘ä»»åŠ¡è´Ÿè½½
        if node.concurrent_tasks < 3:
            score += 0.1
        elif node.concurrent_tasks > 8:
            score -= 0.2
        
        # å†å²æˆåŠŸç‡
        if node.task_completion_rate > 0.9:
            score += 0.2
        elif node.task_completion_rate < 0.7:
            score -= 0.2
        
        return max(0, min(1, score))

class SmartSchedulingEngine:
    """æ™ºèƒ½è°ƒåº¦å¼•æ“"""
    
    def __init__(self):
        self.performance_predictor = NodePerformancePredictor()
        self.task_matcher = TaskNodeMatcher()
        self.historical_data = defaultdict(deque)
        self.training_data = deque(maxlen=1000)
        self.last_training_time = datetime.now()
        self.training_interval = timedelta(hours=6)  # æ¯6å°æ—¶é‡æ–°è®­ç»ƒ
        self._lock = threading.RLock()
        
    async def initialize(self):
        """åˆå§‹åŒ–è°ƒåº¦å¼•æ“"""
        logger.info("ğŸ§  åˆå§‹åŒ–æ™ºèƒ½è°ƒåº¦å¼•æ“...")
        
        # åŠ è½½å†å²æ•°æ®
        await self._load_historical_data()
        
        # åˆå§‹è®­ç»ƒ
        await self._train_models()
        
        # å¯åŠ¨å®šæœŸé‡è®­ç»ƒ
        asyncio.create_task(self._periodic_retraining())
        
        logger.info("âœ… æ™ºèƒ½è°ƒåº¦å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def select_optimal_node(
        self, 
        task: TaskCharacteristics, 
        available_nodes: List[NodePerformanceMetrics]
    ) -> Optional[str]:
        """é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹"""
        if not available_nodes:
            return None
        
        try:
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ç»¼åˆè¯„åˆ†
            node_scores = []
            
            for node in available_nodes:
                # æ€§èƒ½é¢„æµ‹è¯„åˆ†
                performance_score = self.performance_predictor.predict_performance(node)
                
                # åŒ¹é…æ¦‚ç‡è¯„åˆ†
                match_probability = self.task_matcher.predict_match_probability(task, node)
                
                # ç»¼åˆè¯„åˆ† (æ€§èƒ½å 60%ï¼ŒåŒ¹é…åº¦å 40%)
                combined_score = performance_score * 0.6 + match_probability * 100 * 0.4
                
                node_scores.append((node.node_id, combined_score))
            
            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„èŠ‚ç‚¹
            node_scores.sort(key=lambda x: x[1], reverse=True)
            
            best_node_id = node_scores[0][0]
            best_score = node_scores[0][1]
            
            logger.info(f"ğŸ¯ æ™ºèƒ½è°ƒåº¦é€‰æ‹©èŠ‚ç‚¹: {best_node_id} (è¯„åˆ†: {best_score:.2f})")
            return best_node_id
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½èŠ‚ç‚¹é€‰æ‹©å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•é€‰æ‹©ç­–ç•¥
            return available_nodes[0].node_id
    
    async def record_execution_result(
        self, 
        task: TaskCharacteristics, 
        node: NodePerformanceMetrics, 
        success: bool,
        execution_time: float
    ):
        """è®°å½•æ‰§è¡Œç»“æœç”¨äºå­¦ä¹ """
        with self._lock:
            # è®°å½•è®­ç»ƒæ•°æ®
            self.training_data.append((task, node, success))
            
            # è®°å½•æ€§èƒ½æ•°æ®
            self.historical_data[node.node_id].append({
                'timestamp': datetime.now(),
                'metrics': node,
                'execution_time': execution_time,
                'success': success
            })
            
            # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
            if len(self.historical_data[node.node_id]) > 1000:
                self.historical_data[node.node_id].popleft()
    
    async def get_scheduling_insights(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦æ´å¯Ÿ"""
        insights = {
            'total_training_samples': len(self.training_data),
            'models_trained': {
                'performance_predictor': self.performance_predictor.is_trained,
                'task_matcher': self.task_matcher.is_trained
            },
            'last_training_time': self.last_training_time.isoformat(),
            'node_performance_history': {}
        }
        
        # èŠ‚ç‚¹æ€§èƒ½ç»Ÿè®¡
        for node_id, history in self.historical_data.items():
            if history:
                recent_data = list(history)[-10:]  # æœ€è¿‘10æ¡è®°å½•
                success_rate = sum(1 for d in recent_data if d['success']) / len(recent_data)
                avg_execution_time = sum(d['execution_time'] for d in recent_data) / len(recent_data)
                
                insights['node_performance_history'][node_id] = {
                    'recent_success_rate': success_rate,
                    'avg_execution_time': avg_execution_time,
                    'total_samples': len(history)
                }
        
        return insights
    
    async def _load_historical_data(self):
        """åŠ è½½å†å²æ•°æ®"""
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®
        # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        logger.info("ğŸ“Š åŠ è½½å†å²è°ƒåº¦æ•°æ®...")
        
        # ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿçš„å†å²æ•°æ®ç”¨äºåˆå§‹è®­ç»ƒ
        await self._generate_mock_training_data()
    
    async def _generate_mock_training_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
        import random
        
        task_types = ["unit_test", "integration_test", "ui_test", "performance_test"]
        test_levels = [f"level{i}" for i in range(1, 11)]
        
        for _ in range(100):
            # æ¨¡æ‹Ÿä»»åŠ¡ç‰¹å¾
            task = TaskCharacteristics(
                task_type=random.choice(task_types),
                test_level=random.choice(test_levels),
                estimated_duration=random.uniform(30, 600),
                resource_requirements={
                    "cpu_cores": random.randint(1, 8),
                    "memory_gb": random.randint(2, 16)
                },
                priority=random.randint(1, 5),
                dependencies=[]
            )
            
            # æ¨¡æ‹ŸèŠ‚ç‚¹æŒ‡æ ‡
            node = NodePerformanceMetrics(
                node_id=f"node_{random.randint(1, 10)}",
                timestamp=datetime.now(),
                cpu_usage=random.uniform(10, 90),
                memory_usage=random.uniform(20, 80),
                disk_io=random.uniform(0, 100),
                network_io=random.uniform(0, 100),
                task_completion_rate=random.uniform(0.7, 1.0),
                average_execution_time=random.uniform(60, 300),
                error_rate=random.uniform(0, 0.1),
                concurrent_tasks=random.randint(0, 10)
            )
            
            # æ¨¡æ‹Ÿæ‰§è¡Œç»“æœ (æ€§èƒ½å¥½çš„èŠ‚ç‚¹æˆåŠŸç‡æ›´é«˜)
            success_probability = (100 - node.cpu_usage) / 100 * node.task_completion_rate
            success = random.random() < success_probability
            
            self.training_data.append((task, node, success))
    
    async def _train_models(self):
        """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹"""
        if len(self.training_data) < 20:
            logger.warning("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒ")
            return
        
        logger.info("ğŸ¤– å¼€å§‹è®­ç»ƒæ™ºèƒ½è°ƒåº¦æ¨¡å‹...")
        
        try:
            # å‡†å¤‡æ€§èƒ½é¢„æµ‹è®­ç»ƒæ•°æ®
            performance_data = []
            performance_targets = []
            
            # å‡†å¤‡åŒ¹é…æ¨¡å‹è®­ç»ƒæ•°æ®
            matching_data = list(self.training_data)
            
            for task, node, success in self.training_data:
                performance_data.append(node)
                # ç›®æ ‡æ˜¯åŸºäºæˆåŠŸç‡å’Œæ‰§è¡Œæ—¶é—´çš„ç»¼åˆæ€§èƒ½è¯„åˆ†
                performance_score = 100 * node.task_completion_rate - node.average_execution_time / 10
                performance_targets.append(performance_score)
            
            # è®­ç»ƒæ€§èƒ½é¢„æµ‹æ¨¡å‹
            await asyncio.get_event_loop().run_in_executor(
                None, 
                self.performance_predictor.train, 
                performance_data, 
                performance_targets
            )
            
            # è®­ç»ƒä»»åŠ¡åŒ¹é…æ¨¡å‹
            await asyncio.get_event_loop().run_in_executor(
                None,
                self.task_matcher.train,
                matching_data
            )
            
            self.last_training_time = datetime.now()
            logger.info("âœ… æ™ºèƒ½è°ƒåº¦æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    async def _periodic_retraining(self):
        """å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹"""
        while True:
            try:
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                
                if datetime.now() - self.last_training_time > self.training_interval:
                    if len(self.training_data) >= 50:  # æœ‰è¶³å¤Ÿæ–°æ•°æ®æ—¶æ‰é‡æ–°è®­ç»ƒ
                        await self._train_models()
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸé‡è®­ç»ƒå¤±è´¥: {e}")
                await asyncio.sleep(1800)  # å‡ºé”™æ—¶ç­‰å¾…30åˆ†é’Ÿå†è¯•

# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'SmartSchedulingEngine',
    'NodePerformanceMetrics', 
    'TaskCharacteristics',
    'NodePerformancePredictor',
    'TaskNodeMatcher'
]

