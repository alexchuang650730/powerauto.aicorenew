#!/usr/bin/env python3
"""
PowerAutomation v0.574 - é»‘ç™½ç›’æ¨¡æ¿è±å¯Œç³»çµ±

æ ¸å¿ƒæŠ€è¡“é›£é»1è§£æ±ºæ–¹æ¡ˆï¼š
é€éé»‘ç›’åŠç™½ç›’æ–‡æœ¬æè¿°ä¾†è±å¯Œæˆ‘å€‘çš„æ¨¡æ¿

æŠ€è¡“æ¶æ§‹ï¼š
1. æ–‡æœ¬åˆ†æå¼•æ“ - è§£æé»‘ç™½ç›’æè¿°
2. æ¨¡æ¿å­¸ç¿’ç³»çµ± - å¾æè¿°ä¸­æå–æ¨¡æ¿ç‰¹å¾µ
3. æ¨¡æ¿è±å¯Œå¼•æ“ - è‡ªå‹•ç”Ÿæˆå’Œå„ªåŒ–æ¨¡æ¿
4. çŸ¥è­˜åœ–è­œæ§‹å»º - å»ºç«‹æ¨¡æ¿é–“çš„é—œè¯é—œä¿‚
"""

import os
import re
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import hashlib
import pickle
from collections import defaultdict, Counter
import numpy as np

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BoxType(Enum):
    """ç›’å­é¡å‹æšèˆ‰"""
    BLACK_BOX = "black_box"  # é»‘ç›’ï¼šåªçŸ¥é“è¼¸å…¥è¼¸å‡ºï¼Œä¸çŸ¥é“å…§éƒ¨å¯¦ç¾
    WHITE_BOX = "white_box"  # ç™½ç›’ï¼šçŸ¥é“å…§éƒ¨å¯¦ç¾ç´°ç¯€
    GRAY_BOX = "gray_box"    # ç°ç›’ï¼šéƒ¨åˆ†äº†è§£å…§éƒ¨å¯¦ç¾

class TemplateCategory(Enum):
    """æ¨¡æ¿é¡åˆ¥æšèˆ‰"""
    FUNCTIONAL = "functional"        # åŠŸèƒ½æ€§æ¨¡æ¿
    PERFORMANCE = "performance"      # æ€§èƒ½æ¸¬è©¦æ¨¡æ¿
    SECURITY = "security"           # å®‰å…¨æ¸¬è©¦æ¨¡æ¿
    INTEGRATION = "integration"     # é›†æˆæ¸¬è©¦æ¨¡æ¿
    UI_UX = "ui_ux"                # ç”¨æˆ¶ç•Œé¢æ¸¬è©¦æ¨¡æ¿
    API = "api"                    # APIæ¸¬è©¦æ¨¡æ¿
    DATABASE = "database"          # æ•¸æ“šåº«æ¸¬è©¦æ¨¡æ¿

@dataclass
class TextDescription:
    """æ–‡æœ¬æè¿°æ•¸æ“šçµæ§‹"""
    description_id: str
    box_type: BoxType
    category: TemplateCategory
    title: str
    content: str
    input_description: str
    output_description: str
    implementation_details: Optional[str] = None  # ç™½ç›’æ‰æœ‰
    constraints: List[str] = None
    examples: List[Dict[str, Any]] = None
    tags: List[str] = None
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.constraints is None:
            self.constraints = []
        if self.examples is None:
            self.examples = []
        if self.tags is None:
            self.tags = []

@dataclass
class TemplatePattern:
    """æ¨¡æ¿æ¨¡å¼"""
    pattern_id: str
    pattern_type: str
    pattern_content: str
    confidence: float
    frequency: int
    related_patterns: List[str] = None
    
    def __post_init__(self):
        if self.related_patterns is None:
            self.related_patterns = []

@dataclass
class EnrichedTemplate:
    """è±å¯ŒåŒ–çš„æ¨¡æ¿"""
    template_id: str
    original_description: TextDescription
    extracted_patterns: List[TemplatePattern]
    generated_variations: List[Dict[str, Any]]
    test_cases: List[Dict[str, Any]]
    quality_score: float
    enrichment_metadata: Dict[str, Any]
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class TextAnalysisEngine:
    """æ–‡æœ¬åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.keyword_patterns = self._load_keyword_patterns()
        self.structure_patterns = self._load_structure_patterns()
        self.domain_knowledge = self._load_domain_knowledge()
    
    def _load_keyword_patterns(self) -> Dict[str, List[str]]:
        """åŠ è¼‰é—œéµè©æ¨¡å¼"""
        return {
            "input_keywords": [
                "è¼¸å…¥", "åƒæ•¸", "æ•¸æ“š", "è«‹æ±‚", "èª¿ç”¨", "å‚³å…¥", "æ¥æ”¶",
                "input", "parameter", "data", "request", "call", "receive"
            ],
            "output_keywords": [
                "è¼¸å‡º", "è¿”å›", "çµæœ", "éŸ¿æ‡‰", "å›å‚³", "ç”¢ç”Ÿ", "ç”Ÿæˆ",
                "output", "return", "result", "response", "generate", "produce"
            ],
            "process_keywords": [
                "è™•ç†", "åŸ·è¡Œ", "é‹è¡Œ", "è¨ˆç®—", "è½‰æ›", "é©—è­‰", "æª¢æŸ¥",
                "process", "execute", "run", "calculate", "transform", "validate", "check"
            ],
            "condition_keywords": [
                "å¦‚æœ", "ç•¶", "æ¢ä»¶", "æƒ…æ³", "ç‹€æ…‹", "æ¨¡å¼",
                "if", "when", "condition", "case", "state", "mode"
            ],
            "constraint_keywords": [
                "é™åˆ¶", "ç´„æŸ", "è¦å‰‡", "è¦æ±‚", "æ¨™æº–", "è¦ç¯„",
                "constraint", "limitation", "rule", "requirement", "standard", "specification"
            ]
        }
    
    def _load_structure_patterns(self) -> Dict[str, str]:
        """åŠ è¼‰çµæ§‹æ¨¡å¼"""
        return {
            "input_output_pattern": r"è¼¸å…¥[:ï¼š]\s*(.+?)\s*è¼¸å‡º[:ï¼š]\s*(.+)",
            "step_pattern": r"æ­¥é©Ÿ\s*(\d+)[:ï¼š]\s*(.+)",
            "condition_pattern": r"(å¦‚æœ|ç•¶|è‹¥)\s*(.+?)\s*(å‰‡|é‚£éº¼)\s*(.+)",
            "example_pattern": r"ä¾‹å¦‚[:ï¼š]\s*(.+)",
            "constraint_pattern": r"(é™åˆ¶|ç´„æŸ|è¦æ±‚)[:ï¼š]\s*(.+)"
        }
    
    def _load_domain_knowledge(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è¼‰é ˜åŸŸçŸ¥è­˜"""
        return {
            "web_testing": {
                "common_inputs": ["URL", "è¡¨å–®æ•¸æ“š", "ç”¨æˆ¶æ†‘è­‰", "HTTPé ­"],
                "common_outputs": ["é é¢å…§å®¹", "ç‹€æ…‹ç¢¼", "éŸ¿æ‡‰æ™‚é–“", "éŒ¯èª¤ä¿¡æ¯"],
                "common_actions": ["é»æ“Š", "è¼¸å…¥", "æäº¤", "å°èˆª", "é©—è­‰"]
            },
            "api_testing": {
                "common_inputs": ["è«‹æ±‚åƒæ•¸", "èªè­‰ä»¤ç‰Œ", "è«‹æ±‚é«”", "æŸ¥è©¢åƒæ•¸"],
                "common_outputs": ["JSONéŸ¿æ‡‰", "ç‹€æ…‹ç¢¼", "éŸ¿æ‡‰é ­", "éŒ¯èª¤ç¢¼"],
                "common_actions": ["GET", "POST", "PUT", "DELETE", "é©—è­‰éŸ¿æ‡‰"]
            },
            "database_testing": {
                "common_inputs": ["SQLæŸ¥è©¢", "æ•¸æ“šè¨˜éŒ„", "é€£æ¥åƒæ•¸", "äº‹å‹™"],
                "common_outputs": ["æŸ¥è©¢çµæœ", "å½±éŸ¿è¡Œæ•¸", "åŸ·è¡Œæ™‚é–“", "éŒ¯èª¤ä¿¡æ¯"],
                "common_actions": ["æŸ¥è©¢", "æ’å…¥", "æ›´æ–°", "åˆªé™¤", "é©—è­‰æ•¸æ“š"]
            }
        }
    
    def analyze_description(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬æè¿°"""
        logger.info(f"ğŸ” åˆ†ææ–‡æœ¬æè¿°: {description.title}")
        
        analysis_result = {
            "input_analysis": self._analyze_input_section(description),
            "output_analysis": self._analyze_output_section(description),
            "process_analysis": self._analyze_process_section(description),
            "structure_analysis": self._analyze_structure(description),
            "domain_analysis": self._analyze_domain(description),
            "complexity_analysis": self._analyze_complexity(description)
        }
        
        return analysis_result
    
    def _analyze_input_section(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†æè¼¸å…¥éƒ¨åˆ†"""
        input_text = description.input_description
        
        # æå–è¼¸å…¥é¡å‹
        input_types = []
        for keyword in self.keyword_patterns["input_keywords"]:
            if keyword in input_text.lower():
                input_types.append(keyword)
        
        # æå–åƒæ•¸ä¿¡æ¯
        parameters = self._extract_parameters(input_text)
        
        # åˆ†ææ•¸æ“šé¡å‹
        data_types = self._extract_data_types(input_text)
        
        return {
            "input_types": input_types,
            "parameters": parameters,
            "data_types": data_types,
            "complexity": len(parameters)
        }
    
    def _analyze_output_section(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†æè¼¸å‡ºéƒ¨åˆ†"""
        output_text = description.output_description
        
        # æå–è¼¸å‡ºé¡å‹
        output_types = []
        for keyword in self.keyword_patterns["output_keywords"]:
            if keyword in output_text.lower():
                output_types.append(keyword)
        
        # æå–è¿”å›å€¼ä¿¡æ¯
        return_values = self._extract_return_values(output_text)
        
        # åˆ†æè¼¸å‡ºæ ¼å¼
        output_formats = self._extract_output_formats(output_text)
        
        return {
            "output_types": output_types,
            "return_values": return_values,
            "output_formats": output_formats,
            "complexity": len(return_values)
        }
    
    def _analyze_process_section(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†æè™•ç†éç¨‹éƒ¨åˆ†"""
        content = description.content
        implementation = description.implementation_details or ""
        
        # æå–è™•ç†æ­¥é©Ÿ
        steps = self._extract_steps(content + " " + implementation)
        
        # æå–æ¢ä»¶é‚è¼¯
        conditions = self._extract_conditions(content + " " + implementation)
        
        # åˆ†æç®—æ³•è¤‡é›œåº¦
        algorithm_complexity = self._analyze_algorithm_complexity(implementation)
        
        return {
            "steps": steps,
            "conditions": conditions,
            "algorithm_complexity": algorithm_complexity,
            "has_loops": "å¾ªç’°" in content or "loop" in content.lower(),
            "has_recursion": "éæ­¸" in content or "recursion" in content.lower()
        }
    
    def _analyze_structure(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çµæ§‹"""
        full_text = f"{description.content} {description.input_description} {description.output_description}"
        
        structure_info = {}
        
        # æª¢æŸ¥å„ç¨®çµæ§‹æ¨¡å¼
        for pattern_name, pattern in self.structure_patterns.items():
            matches = re.findall(pattern, full_text, re.IGNORECASE)
            if matches:
                structure_info[pattern_name] = matches
        
        return structure_info
    
    def _analyze_domain(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†æé ˜åŸŸç‰¹å¾µ"""
        full_text = f"{description.title} {description.content}".lower()
        
        domain_scores = {}
        for domain, knowledge in self.domain_knowledge.items():
            score = 0
            total_terms = 0
            
            for category, terms in knowledge.items():
                for term in terms:
                    total_terms += 1
                    if term.lower() in full_text:
                        score += 1
            
            domain_scores[domain] = score / total_terms if total_terms > 0 else 0
        
        # æ‰¾å‡ºæœ€åŒ¹é…çš„é ˜åŸŸ
        best_domain = max(domain_scores.items(), key=lambda x: x[1])
        
        return {
            "domain_scores": domain_scores,
            "primary_domain": best_domain[0],
            "domain_confidence": best_domain[1]
        }
    
    def _analyze_complexity(self, description: TextDescription) -> Dict[str, Any]:
        """åˆ†æè¤‡é›œåº¦"""
        content_length = len(description.content)
        num_examples = len(description.examples)
        num_constraints = len(description.constraints)
        
        # è¨ˆç®—æ–‡æœ¬è¤‡é›œåº¦
        text_complexity = min(content_length / 1000, 1.0)  # æ¨™æº–åŒ–åˆ°0-1
        
        # è¨ˆç®—é‚è¼¯è¤‡é›œåº¦
        logic_keywords = ["å¦‚æœ", "å¦å‰‡", "å¾ªç’°", "éæ­¸", "if", "else", "loop", "recursion"]
        logic_count = sum(1 for keyword in logic_keywords if keyword in description.content.lower())
        logic_complexity = min(logic_count / 10, 1.0)  # æ¨™æº–åŒ–åˆ°0-1
        
        # è¨ˆç®—æ•´é«”è¤‡é›œåº¦
        overall_complexity = (text_complexity + logic_complexity + num_constraints * 0.1) / 3
        
        return {
            "text_complexity": text_complexity,
            "logic_complexity": logic_complexity,
            "overall_complexity": overall_complexity,
            "complexity_level": self._get_complexity_level(overall_complexity)
        }
    
    def _extract_parameters(self, text: str) -> List[Dict[str, str]]:
        """æå–åƒæ•¸ä¿¡æ¯"""
        parameters = []
        
        # ç°¡å–®çš„åƒæ•¸æå–é‚è¼¯
        param_patterns = [
            r"åƒæ•¸\s*[:ï¼š]\s*(\w+)",
            r"è¼¸å…¥\s*[:ï¼š]\s*(\w+)",
            r"parameter\s*[:ï¼š]\s*(\w+)"
        ]
        
        for pattern in param_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                parameters.append({"name": match, "type": "unknown"})
        
        return parameters
    
    def _extract_data_types(self, text: str) -> List[str]:
        """æå–æ•¸æ“šé¡å‹"""
        type_keywords = [
            "å­—ç¬¦ä¸²", "æ•¸å­—", "æ•´æ•¸", "æµ®é»", "å¸ƒçˆ¾", "æ•¸çµ„", "å°è±¡", "JSON",
            "string", "number", "integer", "float", "boolean", "array", "object", "json"
        ]
        
        found_types = []
        for type_keyword in type_keywords:
            if type_keyword.lower() in text.lower():
                found_types.append(type_keyword)
        
        return found_types
    
    def _extract_return_values(self, text: str) -> List[Dict[str, str]]:
        """æå–è¿”å›å€¼ä¿¡æ¯"""
        return_values = []
        
        # ç°¡å–®çš„è¿”å›å€¼æå–é‚è¼¯
        return_patterns = [
            r"è¿”å›\s*[:ï¼š]\s*(\w+)",
            r"è¼¸å‡º\s*[:ï¼š]\s*(\w+)",
            r"return\s*[:ï¼š]\s*(\w+)"
        ]
        
        for pattern in return_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                return_values.append({"name": match, "type": "unknown"})
        
        return return_values
    
    def _extract_output_formats(self, text: str) -> List[str]:
        """æå–è¼¸å‡ºæ ¼å¼"""
        format_keywords = [
            "JSON", "XML", "HTML", "CSV", "PDF", "åœ–ç‰‡", "æ–‡æœ¬",
            "json", "xml", "html", "csv", "pdf", "image", "text"
        ]
        
        found_formats = []
        for format_keyword in format_keywords:
            if format_keyword.lower() in text.lower():
                found_formats.append(format_keyword)
        
        return found_formats
    
    def _extract_steps(self, text: str) -> List[str]:
        """æå–è™•ç†æ­¥é©Ÿ"""
        steps = []
        
        # æŸ¥æ‰¾ç·¨è™Ÿæ­¥é©Ÿ
        step_pattern = r"æ­¥é©Ÿ\s*(\d+)[:ï¼š]\s*(.+?)(?=æ­¥é©Ÿ\s*\d+|$)"
        matches = re.findall(step_pattern, text, re.IGNORECASE | re.DOTALL)
        
        for step_num, step_content in matches:
            steps.append(f"æ­¥é©Ÿ{step_num}: {step_content.strip()}")
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç·¨è™Ÿæ­¥é©Ÿï¼Œå˜—è©¦å…¶ä»–æ¨¡å¼
        if not steps:
            # æŸ¥æ‰¾ä»¥æ•¸å­—é–‹é ­çš„è¡Œ
            numbered_lines = re.findall(r"^\d+\.\s*(.+)$", text, re.MULTILINE)
            steps.extend(numbered_lines)
        
        return steps
    
    def _extract_conditions(self, text: str) -> List[str]:
        """æå–æ¢ä»¶é‚è¼¯"""
        conditions = []
        
        condition_patterns = [
            r"(å¦‚æœ|è‹¥|ç•¶)\s*(.+?)\s*(å‰‡|é‚£éº¼)\s*(.+)",
            r"if\s*(.+?)\s*then\s*(.+)",
            r"when\s*(.+?)\s*do\s*(.+)"
        ]
        
        for pattern in condition_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) >= 2:
                    conditions.append(f"æ¢ä»¶: {match[1]} -> å‹•ä½œ: {match[-1]}")
        
        return conditions
    
    def _analyze_algorithm_complexity(self, implementation: str) -> str:
        """åˆ†æç®—æ³•è¤‡é›œåº¦"""
        if not implementation:
            return "unknown"
        
        impl_lower = implementation.lower()
        
        # ç°¡å–®çš„è¤‡é›œåº¦åˆ†æ
        if "nested loop" in impl_lower or "é›™é‡å¾ªç’°" in impl_lower:
            return "O(nÂ²)"
        elif "loop" in impl_lower or "å¾ªç’°" in impl_lower:
            return "O(n)"
        elif "recursion" in impl_lower or "éæ­¸" in impl_lower:
            return "O(log n) or higher"
        else:
            return "O(1)"
    
    def _get_complexity_level(self, complexity_score: float) -> str:
        """ç²å–è¤‡é›œåº¦ç­‰ç´š"""
        if complexity_score < 0.3:
            return "ç°¡å–®"
        elif complexity_score < 0.6:
            return "ä¸­ç­‰"
        else:
            return "è¤‡é›œ"

class TemplateLearningSystem:
    """æ¨¡æ¿å­¸ç¿’ç³»çµ±"""
    
    def __init__(self):
        self.pattern_database = {}
        self.template_cache = {}
        self.learning_history = []
    
    def learn_from_description(self, description: TextDescription, 
                             analysis_result: Dict[str, Any]) -> List[TemplatePattern]:
        """å¾æè¿°ä¸­å­¸ç¿’æ¨¡æ¿æ¨¡å¼"""
        logger.info(f"ğŸ“š å¾æè¿°ä¸­å­¸ç¿’æ¨¡æ¿: {description.title}")
        
        patterns = []
        
        # å­¸ç¿’è¼¸å…¥æ¨¡å¼
        input_patterns = self._learn_input_patterns(description, analysis_result)
        patterns.extend(input_patterns)
        
        # å­¸ç¿’è¼¸å‡ºæ¨¡å¼
        output_patterns = self._learn_output_patterns(description, analysis_result)
        patterns.extend(output_patterns)
        
        # å­¸ç¿’è™•ç†æ¨¡å¼
        process_patterns = self._learn_process_patterns(description, analysis_result)
        patterns.extend(process_patterns)
        
        # å­¸ç¿’çµæ§‹æ¨¡å¼
        structure_patterns = self._learn_structure_patterns(description, analysis_result)
        patterns.extend(structure_patterns)
        
        # æ›´æ–°æ¨¡å¼æ•¸æ“šåº«
        self._update_pattern_database(patterns)
        
        return patterns
    
    def _learn_input_patterns(self, description: TextDescription, 
                            analysis: Dict[str, Any]) -> List[TemplatePattern]:
        """å­¸ç¿’è¼¸å…¥æ¨¡å¼"""
        patterns = []
        input_analysis = analysis.get("input_analysis", {})
        
        # å­¸ç¿’åƒæ•¸æ¨¡å¼
        parameters = input_analysis.get("parameters", [])
        if parameters:
            pattern_content = {
                "parameter_count": len(parameters),
                "parameter_types": [p.get("type", "unknown") for p in parameters],
                "parameter_names": [p.get("name", "") for p in parameters]
            }
            
            pattern = TemplatePattern(
                pattern_id=f"input_param_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="input_parameters",
                pattern_content=json.dumps(pattern_content),
                confidence=0.8,
                frequency=1
            )
            patterns.append(pattern)
        
        # å­¸ç¿’æ•¸æ“šé¡å‹æ¨¡å¼
        data_types = input_analysis.get("data_types", [])
        if data_types:
            pattern_content = {
                "data_types": data_types,
                "type_count": len(data_types)
            }
            
            pattern = TemplatePattern(
                pattern_id=f"input_types_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="input_data_types",
                pattern_content=json.dumps(pattern_content),
                confidence=0.7,
                frequency=1
            )
            patterns.append(pattern)
        
        return patterns
    
    def _learn_output_patterns(self, description: TextDescription, 
                             analysis: Dict[str, Any]) -> List[TemplatePattern]:
        """å­¸ç¿’è¼¸å‡ºæ¨¡å¼"""
        patterns = []
        output_analysis = analysis.get("output_analysis", {})
        
        # å­¸ç¿’è¿”å›å€¼æ¨¡å¼
        return_values = output_analysis.get("return_values", [])
        if return_values:
            pattern_content = {
                "return_count": len(return_values),
                "return_types": [r.get("type", "unknown") for r in return_values],
                "return_names": [r.get("name", "") for r in return_values]
            }
            
            pattern = TemplatePattern(
                pattern_id=f"output_return_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="output_returns",
                pattern_content=json.dumps(pattern_content),
                confidence=0.8,
                frequency=1
            )
            patterns.append(pattern)
        
        # å­¸ç¿’è¼¸å‡ºæ ¼å¼æ¨¡å¼
        output_formats = output_analysis.get("output_formats", [])
        if output_formats:
            pattern_content = {
                "formats": output_formats,
                "format_count": len(output_formats)
            }
            
            pattern = TemplatePattern(
                pattern_id=f"output_formats_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="output_formats",
                pattern_content=json.dumps(pattern_content),
                confidence=0.7,
                frequency=1
            )
            patterns.append(pattern)
        
        return patterns
    
    def _learn_process_patterns(self, description: TextDescription, 
                              analysis: Dict[str, Any]) -> List[TemplatePattern]:
        """å­¸ç¿’è™•ç†æ¨¡å¼"""
        patterns = []
        process_analysis = analysis.get("process_analysis", {})
        
        # å­¸ç¿’æ­¥é©Ÿæ¨¡å¼
        steps = process_analysis.get("steps", [])
        if steps:
            pattern_content = {
                "step_count": len(steps),
                "steps": steps,
                "has_conditions": len(process_analysis.get("conditions", [])) > 0,
                "has_loops": process_analysis.get("has_loops", False),
                "has_recursion": process_analysis.get("has_recursion", False)
            }
            
            pattern = TemplatePattern(
                pattern_id=f"process_steps_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="process_steps",
                pattern_content=json.dumps(pattern_content),
                confidence=0.9,
                frequency=1
            )
            patterns.append(pattern)
        
        # å­¸ç¿’ç®—æ³•è¤‡é›œåº¦æ¨¡å¼
        algorithm_complexity = process_analysis.get("algorithm_complexity", "unknown")
        if algorithm_complexity != "unknown":
            pattern_content = {
                "complexity": algorithm_complexity,
                "box_type": description.box_type.value
            }
            
            pattern = TemplatePattern(
                pattern_id=f"algorithm_complexity_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                pattern_type="algorithm_complexity",
                pattern_content=json.dumps(pattern_content),
                confidence=0.6,
                frequency=1
            )
            patterns.append(pattern)
        
        return patterns
    
    def _learn_structure_patterns(self, description: TextDescription, 
                                analysis: Dict[str, Any]) -> List[TemplatePattern]:
        """å­¸ç¿’çµæ§‹æ¨¡å¼"""
        patterns = []
        structure_analysis = analysis.get("structure_analysis", {})
        
        for structure_type, matches in structure_analysis.items():
            if matches:
                pattern_content = {
                    "structure_type": structure_type,
                    "matches": matches,
                    "match_count": len(matches)
                }
                
                pattern = TemplatePattern(
                    pattern_id=f"structure_{structure_type}_{hashlib.md5(str(pattern_content).encode()).hexdigest()[:8]}",
                    pattern_type=f"structure_{structure_type}",
                    pattern_content=json.dumps(pattern_content),
                    confidence=0.7,
                    frequency=1
                )
                patterns.append(pattern)
        
        return patterns
    
    def _update_pattern_database(self, patterns: List[TemplatePattern]):
        """æ›´æ–°æ¨¡å¼æ•¸æ“šåº«"""
        for pattern in patterns:
            if pattern.pattern_id in self.pattern_database:
                # æ›´æ–°ç¾æœ‰æ¨¡å¼çš„é »ç‡
                existing_pattern = self.pattern_database[pattern.pattern_id]
                existing_pattern.frequency += 1
                existing_pattern.confidence = min(existing_pattern.confidence + 0.1, 1.0)
            else:
                # æ·»åŠ æ–°æ¨¡å¼
                self.pattern_database[pattern.pattern_id] = pattern
        
        logger.info(f"ğŸ“Š æ¨¡å¼æ•¸æ“šåº«å·²æ›´æ–°ï¼Œç•¶å‰åŒ…å« {len(self.pattern_database)} å€‹æ¨¡å¼")

class TemplateEnrichmentEngine:
    """æ¨¡æ¿è±å¯Œå¼•æ“"""
    
    def __init__(self, text_analyzer: TextAnalysisEngine, learning_system: TemplateLearningSystem):
        self.text_analyzer = text_analyzer
        self.learning_system = learning_system
        self.enrichment_strategies = self._load_enrichment_strategies()
    
    def _load_enrichment_strategies(self) -> Dict[str, Callable]:
        """åŠ è¼‰è±å¯ŒåŒ–ç­–ç•¥"""
        return {
            "variation_generation": self._generate_variations,
            "test_case_generation": self._generate_test_cases,
            "error_case_generation": self._generate_error_test_cases,
            "performance_scenarios": self._generate_performance_test_cases
        }
    
    def enrich_template(self, description: TextDescription) -> EnrichedTemplate:
        """è±å¯ŒåŒ–æ¨¡æ¿"""
        logger.info(f"ğŸ”§ é–‹å§‹è±å¯ŒåŒ–æ¨¡æ¿: {description.title}")
        
        # åˆ†ææ–‡æœ¬æè¿°
        analysis_result = self.text_analyzer.analyze_description(description)
        
        # å­¸ç¿’æ¨¡å¼
        patterns = self.learning_system.learn_from_description(description, analysis_result)
        
        # ç”Ÿæˆè®Šé«”
        variations = self._generate_variations(description, analysis_result, patterns)
        
        # ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
        test_cases = self._generate_test_cases(description, analysis_result, patterns)
        
        # è¨ˆç®—è³ªé‡è©•åˆ†
        quality_score = self._calculate_quality_score(description, analysis_result, patterns)
        
        # å‰µå»ºè±å¯ŒåŒ–æ¨¡æ¿
        enriched_template = EnrichedTemplate(
            template_id=f"enriched_{hashlib.md5(description.content.encode()).hexdigest()[:8]}",
            original_description=description,
            extracted_patterns=patterns,
            generated_variations=variations,
            test_cases=test_cases,
            quality_score=quality_score,
            enrichment_metadata={
                "analysis_result": analysis_result,
                "enrichment_timestamp": datetime.now().isoformat(),
                "pattern_count": len(patterns),
                "variation_count": len(variations),
                "test_case_count": len(test_cases)
            }
        )
        
        logger.info(f"âœ… æ¨¡æ¿è±å¯ŒåŒ–å®Œæˆï¼Œè³ªé‡è©•åˆ†: {quality_score:.2f}")
        return enriched_template
    
    def _generate_variations(self, description: TextDescription, 
                           analysis: Dict[str, Any], 
                           patterns: List[TemplatePattern]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ¿è®Šé«”"""
        variations = []
        
        # åŸºæ–¼è¼¸å…¥åƒæ•¸ç”Ÿæˆè®Šé«”
        input_analysis = analysis.get("input_analysis", {})
        parameters = input_analysis.get("parameters", [])
        
        if parameters:
            # åƒæ•¸æ•¸é‡è®Šé«”
            variations.append({
                "type": "parameter_count_variation",
                "description": "æ¸›å°‘åƒæ•¸æ•¸é‡çš„ç°¡åŒ–ç‰ˆæœ¬",
                "changes": {"parameter_count": max(1, len(parameters) - 1)},
                "confidence": 0.7
            })
            
            variations.append({
                "type": "parameter_count_variation",
                "description": "å¢åŠ åƒæ•¸æ•¸é‡çš„æ“´å±•ç‰ˆæœ¬",
                "changes": {"parameter_count": len(parameters) + 1},
                "confidence": 0.6
            })
        
        # åŸºæ–¼è¤‡é›œåº¦ç”Ÿæˆè®Šé«”
        complexity_analysis = analysis.get("complexity_analysis", {})
        complexity_level = complexity_analysis.get("complexity_level", "ä¸­ç­‰")
        
        if complexity_level == "è¤‡é›œ":
            variations.append({
                "type": "complexity_reduction",
                "description": "ç°¡åŒ–ç‰ˆæœ¬ï¼Œé™ä½è¤‡é›œåº¦",
                "changes": {"complexity_level": "ç°¡å–®"},
                "confidence": 0.8
            })
        elif complexity_level == "ç°¡å–®":
            variations.append({
                "type": "complexity_increase",
                "description": "å¢å¼·ç‰ˆæœ¬ï¼Œæé«˜è¤‡é›œåº¦",
                "changes": {"complexity_level": "ä¸­ç­‰"},
                "confidence": 0.7
            })
        
        # åŸºæ–¼é ˜åŸŸçŸ¥è­˜ç”Ÿæˆè®Šé«”
        domain_analysis = analysis.get("domain_analysis", {})
        primary_domain = domain_analysis.get("primary_domain", "")
        
        if primary_domain:
            variations.append({
                "type": "domain_specific_variation",
                "description": f"é‡å°{primary_domain}é ˜åŸŸçš„å°ˆé–€åŒ–ç‰ˆæœ¬",
                "changes": {"domain_focus": primary_domain},
                "confidence": 0.8
            })
        
        return variations
    
    def _generate_test_cases(self, description: TextDescription, 
                           analysis: Dict[str, Any], 
                           patterns: List[TemplatePattern]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹"""
        test_cases = []
        
        # åŸºæœ¬åŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹
        test_cases.append({
            "type": "basic_functionality",
            "title": "åŸºæœ¬åŠŸèƒ½æ¸¬è©¦",
            "description": f"æ¸¬è©¦{description.title}çš„åŸºæœ¬åŠŸèƒ½",
            "priority": "high",
            "test_data": self._generate_basic_test_data(description, analysis),
            "expected_result": "åŠŸèƒ½æ­£å¸¸åŸ·è¡Œ",
            "confidence": 0.9
        })
        
        # é‚Šç•Œæ¢ä»¶æ¸¬è©¦ç”¨ä¾‹
        boundary_cases = self._generate_boundary_test_cases(description, analysis)
        test_cases.extend(boundary_cases)
        
        # éŒ¯èª¤è™•ç†æ¸¬è©¦ç”¨ä¾‹
        error_cases = self._generate_error_test_cases(description, analysis)
        test_cases.extend(error_cases)
        
        # æ€§èƒ½æ¸¬è©¦ç”¨ä¾‹
        if description.category in [TemplateCategory.PERFORMANCE, TemplateCategory.API]:
            performance_cases = self._generate_performance_test_cases(description, analysis)
            test_cases.extend(performance_cases)
        
        # å®‰å…¨æ¸¬è©¦ç”¨ä¾‹
        if description.category == TemplateCategory.SECURITY:
            security_cases = self._generate_security_test_cases(description, analysis)
            test_cases.extend(security_cases)
        
        return test_cases
    
    def _generate_basic_test_data(self, description: TextDescription, 
                                analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæœ¬æ¸¬è©¦æ•¸æ“š"""
        input_analysis = analysis.get("input_analysis", {})
        parameters = input_analysis.get("parameters", [])
        data_types = input_analysis.get("data_types", [])
        
        test_data = {}
        
        for i, param in enumerate(parameters):
            param_name = param.get("name", f"param_{i}")
            
            # æ ¹æ“šæ•¸æ“šé¡å‹ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
            if "å­—ç¬¦ä¸²" in data_types or "string" in data_types:
                test_data[param_name] = "test_string"
            elif "æ•¸å­—" in data_types or "number" in data_types:
                test_data[param_name] = 123
            elif "å¸ƒçˆ¾" in data_types or "boolean" in data_types:
                test_data[param_name] = True
            else:
                test_data[param_name] = "test_value"
        
        return test_data
    
    def _generate_boundary_test_cases(self, description: TextDescription, 
                                    analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé‚Šç•Œæ¢ä»¶æ¸¬è©¦ç”¨ä¾‹"""
        boundary_cases = []
        
        # ç©ºå€¼æ¸¬è©¦
        boundary_cases.append({
            "type": "boundary_null",
            "title": "ç©ºå€¼é‚Šç•Œæ¸¬è©¦",
            "description": "æ¸¬è©¦ç©ºå€¼è¼¸å…¥çš„è™•ç†",
            "priority": "medium",
            "test_data": {"input": None},
            "expected_result": "æ­£ç¢ºè™•ç†ç©ºå€¼æˆ–æ‹‹å‡ºé©ç•¶éŒ¯èª¤",
            "confidence": 0.8
        })
        
        # æœ€å¤§å€¼æ¸¬è©¦
        boundary_cases.append({
            "type": "boundary_max",
            "title": "æœ€å¤§å€¼é‚Šç•Œæ¸¬è©¦",
            "description": "æ¸¬è©¦æœ€å¤§å€¼è¼¸å…¥çš„è™•ç†",
            "priority": "medium",
            "test_data": {"input": "max_value"},
            "expected_result": "æ­£ç¢ºè™•ç†æœ€å¤§å€¼",
            "confidence": 0.7
        })
        
        # æœ€å°å€¼æ¸¬è©¦
        boundary_cases.append({
            "type": "boundary_min",
            "title": "æœ€å°å€¼é‚Šç•Œæ¸¬è©¦",
            "description": "æ¸¬è©¦æœ€å°å€¼è¼¸å…¥çš„è™•ç†",
            "priority": "medium",
            "test_data": {"input": "min_value"},
            "expected_result": "æ­£ç¢ºè™•ç†æœ€å°å€¼",
            "confidence": 0.7
        })
        
        return boundary_cases
    
    def _generate_error_test_cases(self, description: TextDescription, 
                                 analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”ŸæˆéŒ¯èª¤è™•ç†æ¸¬è©¦ç”¨ä¾‹"""
        error_cases = []
        
        # ç„¡æ•ˆè¼¸å…¥æ¸¬è©¦
        error_cases.append({
            "type": "error_invalid_input",
            "title": "ç„¡æ•ˆè¼¸å…¥æ¸¬è©¦",
            "description": "æ¸¬è©¦ç„¡æ•ˆè¼¸å…¥çš„éŒ¯èª¤è™•ç†",
            "priority": "high",
            "test_data": {"input": "invalid_data"},
            "expected_result": "æ‹‹å‡ºé©ç•¶çš„éŒ¯èª¤ä¿¡æ¯",
            "confidence": 0.8
        })
        
        # é¡å‹éŒ¯èª¤æ¸¬è©¦
        error_cases.append({
            "type": "error_type_mismatch",
            "title": "é¡å‹éŒ¯èª¤æ¸¬è©¦",
            "description": "æ¸¬è©¦é¡å‹ä¸åŒ¹é…çš„éŒ¯èª¤è™•ç†",
            "priority": "medium",
            "test_data": {"input": "wrong_type"},
            "expected_result": "æ‹‹å‡ºé¡å‹éŒ¯èª¤",
            "confidence": 0.7
        })
        
        return error_cases
    
    def _generate_performance_test_cases(self, description: TextDescription, 
                                       analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ€§èƒ½æ¸¬è©¦ç”¨ä¾‹"""
        performance_cases = []
        
        # éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
        performance_cases.append({
            "type": "performance_response_time",
            "title": "éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦",
            "description": "æ¸¬è©¦åŠŸèƒ½çš„éŸ¿æ‡‰æ™‚é–“",
            "priority": "medium",
            "test_data": {"load": "normal"},
            "expected_result": "éŸ¿æ‡‰æ™‚é–“åœ¨å¯æ¥å—ç¯„åœå…§",
            "confidence": 0.7
        })
        
        # è² è¼‰æ¸¬è©¦
        performance_cases.append({
            "type": "performance_load",
            "title": "è² è¼‰æ¸¬è©¦",
            "description": "æ¸¬è©¦é«˜è² è¼‰ä¸‹çš„æ€§èƒ½",
            "priority": "medium",
            "test_data": {"load": "high"},
            "expected_result": "åœ¨é«˜è² è¼‰ä¸‹ä¿æŒç©©å®šæ€§èƒ½",
            "confidence": 0.6
        })
        
        return performance_cases
    
    def _generate_security_test_cases(self, description: TextDescription, 
                                    analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå®‰å…¨æ¸¬è©¦ç”¨ä¾‹"""
        security_cases = []
        
        # SQLæ³¨å…¥æ¸¬è©¦
        security_cases.append({
            "type": "security_sql_injection",
            "title": "SQLæ³¨å…¥æ¸¬è©¦",
            "description": "æ¸¬è©¦SQLæ³¨å…¥æ”»æ“Šçš„é˜²è­·",
            "priority": "high",
            "test_data": {"input": "'; DROP TABLE users; --"},
            "expected_result": "æˆåŠŸé˜²æ­¢SQLæ³¨å…¥æ”»æ“Š",
            "confidence": 0.8
        })
        
        # XSSæ¸¬è©¦
        security_cases.append({
            "type": "security_xss",
            "title": "XSSæ”»æ“Šæ¸¬è©¦",
            "description": "æ¸¬è©¦è·¨ç«™è…³æœ¬æ”»æ“Šçš„é˜²è­·",
            "priority": "high",
            "test_data": {"input": "<script>alert('xss')</script>"},
            "expected_result": "æˆåŠŸé˜²æ­¢XSSæ”»æ“Š",
            "confidence": 0.8
        })
        
        return security_cases
    
    def _calculate_quality_score(self, description: TextDescription, 
                               analysis: Dict[str, Any], 
                               patterns: List[TemplatePattern]) -> float:
        """è¨ˆç®—è³ªé‡è©•åˆ†"""
        score = 0.0
        
        # åŸºæ–¼æè¿°å®Œæ•´æ€§è©•åˆ† (30%)
        completeness_score = 0.0
        if description.input_description:
            completeness_score += 0.3
        if description.output_description:
            completeness_score += 0.3
        if description.implementation_details:
            completeness_score += 0.2
        if description.examples:
            completeness_score += 0.1
        if description.constraints:
            completeness_score += 0.1
        
        score += completeness_score * 0.3
        
        # åŸºæ–¼åˆ†ææ·±åº¦è©•åˆ† (25%)
        analysis_score = 0.0
        complexity_analysis = analysis.get("complexity_analysis", {})
        domain_analysis = analysis.get("domain_analysis", {})
        
        if complexity_analysis.get("overall_complexity", 0) > 0:
            analysis_score += 0.4
        if domain_analysis.get("domain_confidence", 0) > 0.5:
            analysis_score += 0.3
        if analysis.get("structure_analysis", {}):
            analysis_score += 0.3
        
        score += analysis_score * 0.25
        
        # åŸºæ–¼æ¨¡å¼æ•¸é‡è©•åˆ† (20%)
        pattern_score = min(len(patterns) / 10, 1.0)  # æœ€å¤š10å€‹æ¨¡å¼å¾—æ»¿åˆ†
        score += pattern_score * 0.2
        
        # åŸºæ–¼æ¨¡å¼è³ªé‡è©•åˆ† (25%)
        if patterns:
            avg_confidence = sum(p.confidence for p in patterns) / len(patterns)
            score += avg_confidence * 0.25
        
        return min(score, 1.0)

# æ¸¬è©¦å‡½æ•¸
def test_template_enrichment_system():
    """æ¸¬è©¦æ¨¡æ¿è±å¯Œç³»çµ±"""
    print("ğŸ§ª æ¸¬è©¦é»‘ç™½ç›’æ¨¡æ¿è±å¯Œç³»çµ±")
    print("=" * 80)
    
    # å‰µå»ºç³»çµ±çµ„ä»¶
    text_analyzer = TextAnalysisEngine()
    learning_system = TemplateLearningSystem()
    enrichment_engine = TemplateEnrichmentEngine(text_analyzer, learning_system)
    
    # æ¸¬è©¦ç”¨ä¾‹1: é»‘ç›’æè¿°
    black_box_description = TextDescription(
        description_id="test_black_001",
        box_type=BoxType.BLACK_BOX,
        category=TemplateCategory.API,
        title="ç”¨æˆ¶ç™»éŒ„API",
        content="é€™æ˜¯ä¸€å€‹ç”¨æˆ¶ç™»éŒ„çš„APIæ¥å£ï¼Œç”¨æ–¼é©—è­‰ç”¨æˆ¶èº«ä»½",
        input_description="è¼¸å…¥ç”¨æˆ¶åå’Œå¯†ç¢¼",
        output_description="è¿”å›ç™»éŒ„çµæœå’Œç”¨æˆ¶ä¿¡æ¯",
        examples=[
            {"input": {"username": "test", "password": "123456"}, "output": {"success": True, "user_id": 1}}
        ],
        constraints=["ç”¨æˆ¶åä¸èƒ½ç‚ºç©º", "å¯†ç¢¼é•·åº¦è‡³å°‘6ä½"],
        tags=["authentication", "api", "security"]
    )
    
    # æ¸¬è©¦ç”¨ä¾‹2: ç™½ç›’æè¿°
    white_box_description = TextDescription(
        description_id="test_white_001",
        box_type=BoxType.WHITE_BOX,
        category=TemplateCategory.FUNCTIONAL,
        title="æ’åºç®—æ³•å¯¦ç¾",
        content="å¯¦ç¾å¿«é€Ÿæ’åºç®—æ³•å°æ•¸çµ„é€²è¡Œæ’åº",
        input_description="è¼¸å…¥ä¸€å€‹æ•´æ•¸æ•¸çµ„",
        output_description="è¿”å›æ’åºå¾Œçš„æ•¸çµ„",
        implementation_details="""
        æ­¥é©Ÿ1: é¸æ“‡åŸºæº–å…ƒç´ 
        æ­¥é©Ÿ2: åˆ†å‰²æ•¸çµ„ç‚ºå°æ–¼å’Œå¤§æ–¼åŸºæº–çš„å…©éƒ¨åˆ†
        æ­¥é©Ÿ3: éæ­¸æ’åºå…©å€‹å­æ•¸çµ„
        æ­¥é©Ÿ4: åˆä½µçµæœ
        ç®—æ³•è¤‡é›œåº¦: O(n log n)
        """,
        examples=[
            {"input": [3, 1, 4, 1, 5], "output": [1, 1, 3, 4, 5]}
        ],
        constraints=["æ•¸çµ„é•·åº¦ä¸è¶…é10000", "å…ƒç´ ç‚ºæ•´æ•¸"],
        tags=["algorithm", "sorting", "recursion"]
    )
    
    # æ¸¬è©¦é»‘ç›’æ¨¡æ¿è±å¯Œ
    print("\nğŸ” æ¸¬è©¦é»‘ç›’æ¨¡æ¿è±å¯Œ")
    print("-" * 60)
    black_box_enriched = enrichment_engine.enrich_template(black_box_description)
    
    print(f"âœ… é»‘ç›’æ¨¡æ¿è±å¯Œå®Œæˆ")
    print(f"ğŸ“Š è³ªé‡è©•åˆ†: {black_box_enriched.quality_score:.2f}")
    print(f"ğŸ”§ æå–æ¨¡å¼æ•¸: {len(black_box_enriched.extracted_patterns)}")
    print(f"ğŸ”„ ç”Ÿæˆè®Šé«”æ•¸: {len(black_box_enriched.generated_variations)}")
    print(f"ğŸ§ª æ¸¬è©¦ç”¨ä¾‹æ•¸: {len(black_box_enriched.test_cases)}")
    
    # æ¸¬è©¦ç™½ç›’æ¨¡æ¿è±å¯Œ
    print("\nğŸ” æ¸¬è©¦ç™½ç›’æ¨¡æ¿è±å¯Œ")
    print("-" * 60)
    white_box_enriched = enrichment_engine.enrich_template(white_box_description)
    
    print(f"âœ… ç™½ç›’æ¨¡æ¿è±å¯Œå®Œæˆ")
    print(f"ğŸ“Š è³ªé‡è©•åˆ†: {white_box_enriched.quality_score:.2f}")
    print(f"ğŸ”§ æå–æ¨¡å¼æ•¸: {len(white_box_enriched.extracted_patterns)}")
    print(f"ğŸ”„ ç”Ÿæˆè®Šé«”æ•¸: {len(white_box_enriched.generated_variations)}")
    print(f"ğŸ§ª æ¸¬è©¦ç”¨ä¾‹æ•¸: {len(white_box_enriched.test_cases)}")
    
    # é¡¯ç¤ºå­¸ç¿’åˆ°çš„æ¨¡å¼
    print(f"\nğŸ“š å­¸ç¿’ç³»çµ±çµ±è¨ˆ")
    print("-" * 60)
    print(f"æ¨¡å¼æ•¸æ“šåº«å¤§å°: {len(learning_system.pattern_database)}")
    
    for pattern_id, pattern in list(learning_system.pattern_database.items())[:3]:
        print(f"  ğŸ“‹ æ¨¡å¼: {pattern.pattern_type}")
        print(f"     ä¿¡å¿ƒåº¦: {pattern.confidence:.2f}")
        print(f"     é »ç‡: {pattern.frequency}")
    
    print("\nğŸ‰ é»‘ç™½ç›’æ¨¡æ¿è±å¯Œç³»çµ±æ¸¬è©¦å®Œæˆï¼")
    
    return {
        "black_box_result": black_box_enriched,
        "white_box_result": white_box_enriched,
        "learning_system": learning_system
    }

if __name__ == "__main__":
    test_template_enrichment_system()

