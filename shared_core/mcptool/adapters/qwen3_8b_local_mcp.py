"""
Qwen3 8B æœ¬åœ°æ¨¡å‹ MCP é©é…å™¨
æ”¯æŒè·¨å¹³å°éƒ¨ç½² (Windows WSL, macOS, Linux)
"""

import os
import sys
import json
import asyncio
import logging
import platform
import subprocess
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import requests
import time

logger = logging.getLogger(__name__)

class Qwen3LocalModelMCP:
    """Qwen3 8B æœ¬åœ°æ¨¡å‹ MCP é©é…å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.model_name = "qwen2.5:8b"  # Ollamaæ¨¡å‹åç¨±
        self.base_url = self.config.get('base_url', 'http://localhost:11434')
        self.api_endpoint = f"{self.base_url}/api/generate"
        self.chat_endpoint = f"{self.base_url}/api/chat"
        
        # å¹³å°æª¢æ¸¬
        self.platform = platform.system().lower()
        self.is_wsl = self._detect_wsl()
        
        # æ¨¡å‹ç‹€æ…‹
        self.model_loaded = False
        self.ollama_running = False
        
        logger.info(f"Qwen3 8B MCPé©é…å™¨åˆå§‹åŒ– - å¹³å°: {self.platform}, WSL: {self.is_wsl}")
    
    def _detect_wsl(self) -> bool:
        """æª¢æ¸¬æ˜¯å¦åœ¨WSLç’°å¢ƒä¸­é‹è¡Œ"""
        try:
            if self.platform == "linux":
                with open('/proc/version', 'r') as f:
                    return 'microsoft' in f.read().lower()
        except:
            pass
        return False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ç’°å¢ƒ"""
        try:
            # 1. æª¢æŸ¥Ollamaæ˜¯å¦å®‰è£
            if not await self._check_ollama_installed():
                logger.info("Ollamaæœªå®‰è£ï¼Œé–‹å§‹å®‰è£...")
                if not await self._install_ollama():
                    return False
            
            # 2. å•Ÿå‹•Ollamaæœå‹™
            if not await self._start_ollama_service():
                return False
            
            # 3. ä¸‹è¼‰ä¸¦åŠ è¼‰Qwen3æ¨¡å‹
            if not await self._ensure_model_available():
                return False
            
            self.model_loaded = True
            logger.info("Qwen3 8Bæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"Qwen3æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def _check_ollama_installed(self) -> bool:
        """æª¢æŸ¥Ollamaæ˜¯å¦å·²å®‰è£"""
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except:
            return False
    
    async def _install_ollama(self) -> bool:
        """è·¨å¹³å°å®‰è£Ollama"""
        try:
            if self.platform == "darwin":  # macOS
                logger.info("åœ¨macOSä¸Šå®‰è£Ollama...")
                result = subprocess.run([
                    'curl', '-fsSL', 'https://ollama.ai/install.sh'
                ], capture_output=True, text=True)
                if result.returncode == 0:
                    subprocess.run(['sh'], input=result.stdout, text=True)
                    
            elif self.platform == "linux" or self.is_wsl:  # Linux/WSL
                logger.info("åœ¨Linux/WSLä¸Šå®‰è£Ollama...")
                result = subprocess.run([
                    'curl', '-fsSL', 'https://ollama.ai/install.sh'
                ], capture_output=True, text=True)
                if result.returncode == 0:
                    subprocess.run(['sh'], input=result.stdout, text=True)
                    
            elif self.platform == "windows":  # Windows
                logger.info("è«‹æ‰‹å‹•å¾ https://ollama.ai ä¸‹è¼‰Windowsç‰ˆæœ¬")
                return False
            
            # é©—è­‰å®‰è£
            await asyncio.sleep(5)  # ç­‰å¾…å®‰è£å®Œæˆ
            return await self._check_ollama_installed()
            
        except Exception as e:
            logger.error(f"Ollamaå®‰è£å¤±æ•—: {e}")
            return False
    
    async def _start_ollama_service(self) -> bool:
        """å•Ÿå‹•Ollamaæœå‹™"""
        try:
            # æª¢æŸ¥æœå‹™æ˜¯å¦å·²é‹è¡Œ
            if await self._check_ollama_running():
                self.ollama_running = True
                return True
            
            # å•Ÿå‹•æœå‹™
            logger.info("å•Ÿå‹•Ollamaæœå‹™...")
            if self.platform == "windows":
                subprocess.Popen(['ollama', 'serve'], 
                               creationflags=subprocess.CREATE_NEW_CONSOLE)
            else:
                subprocess.Popen(['ollama', 'serve'], 
                               stdout=subprocess.DEVNULL, 
                               stderr=subprocess.DEVNULL)
            
            # ç­‰å¾…æœå‹™å•Ÿå‹•
            for _ in range(30):  # æœ€å¤šç­‰å¾…30ç§’
                await asyncio.sleep(1)
                if await self._check_ollama_running():
                    self.ollama_running = True
                    logger.info("Ollamaæœå‹™å•Ÿå‹•æˆåŠŸ")
                    return True
            
            logger.error("Ollamaæœå‹™å•Ÿå‹•è¶…æ™‚")
            return False
            
        except Exception as e:
            logger.error(f"Ollamaæœå‹™å•Ÿå‹•å¤±æ•—: {e}")
            return False
    
    async def _check_ollama_running(self) -> bool:
        """æª¢æŸ¥Ollamaæœå‹™æ˜¯å¦é‹è¡Œ"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def _ensure_model_available(self) -> bool:
        """ç¢ºä¿Qwen3æ¨¡å‹å¯ç”¨"""
        try:
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get('models', [])
                for model in models:
                    if self.model_name in model.get('name', ''):
                        logger.info(f"æ¨¡å‹ {self.model_name} å·²å­˜åœ¨")
                        return True
            
            # ä¸‹è¼‰æ¨¡å‹
            logger.info(f"ä¸‹è¼‰æ¨¡å‹ {self.model_name}...")
            result = subprocess.run([
                'ollama', 'pull', self.model_name
            ], capture_output=True, text=True, timeout=600)  # 10åˆ†é˜è¶…æ™‚
            
            if result.returncode == 0:
                logger.info(f"æ¨¡å‹ {self.model_name} ä¸‹è¼‰æˆåŠŸ")
                return True
            else:
                logger.error(f"æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"æ¨¡å‹æº–å‚™å¤±æ•—: {e}")
            return False
    
    async def generate_response(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå›æ‡‰"""
        try:
            if not self.model_loaded:
                await self.initialize()
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                **kwargs
            }
            
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response": result.get('response', ''),
                    "model": self.model_name,
                    "done": result.get('done', True),
                    "context": result.get('context', [])
                }
            else:
                return {
                    "success": False,
                    "error": f"APIè«‹æ±‚å¤±æ•—: {response.status_code}",
                    "details": response.text
                }
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """èŠå¤©å®Œæˆ"""
        try:
            if not self.model_loaded:
                await self.initialize()
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "stream": False,
                **kwargs
            }
            
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "message": result.get('message', {}),
                    "model": self.model_name,
                    "done": result.get('done', True)
                }
            else:
                return {
                    "success": False,
                    "error": f"èŠå¤©APIè«‹æ±‚å¤±æ•—: {response.status_code}",
                    "details": response.text
                }
                
        except Exception as e:
            logger.error(f"èŠå¤©å®Œæˆå¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": self.model_name},
                timeout=10
            )
            
            if response.status_code == 200:
                return {
                    "success": True,
                    "model_info": response.json()
                }
            else:
                return {
                    "success": False,
                    "error": f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {response.status_code}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        try:
            # æª¢æŸ¥Ollamaæœå‹™
            ollama_status = await self._check_ollama_running()
            
            # æª¢æŸ¥æ¨¡å‹ç‹€æ…‹
            model_info = await self.get_model_info()
            
            return {
                "success": True,
                "status": {
                    "ollama_running": ollama_status,
                    "model_loaded": self.model_loaded,
                    "model_available": model_info.get("success", False),
                    "platform": self.platform,
                    "is_wsl": self.is_wsl,
                    "base_url": self.base_url,
                    "model_name": self.model_name
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_adapter_info(self) -> Dict[str, Any]:
        """ç²å–é©é…å™¨ä¿¡æ¯"""
        return {
            "name": "Qwen3 8B Local Model MCP",
            "version": "1.0.0",
            "description": "Qwen3 8Bæœ¬åœ°æ¨¡å‹é©é…å™¨ï¼Œæ”¯æŒè·¨å¹³å°éƒ¨ç½²",
            "supported_platforms": ["Windows WSL", "macOS", "Linux"],
            "model": self.model_name,
            "inference_engine": "Ollama",
            "capabilities": [
                "text_generation",
                "chat_completion", 
                "cross_platform_deployment",
                "local_inference"
            ]
        }

# MCPé©é…å™¨è¨»å†Š
def create_adapter(config: Dict[str, Any] = None) -> Qwen3LocalModelMCP:
    """å‰µå»ºQwen3 8Bé©é…å™¨å¯¦ä¾‹"""
    return Qwen3LocalModelMCP(config)

# é©é…å™¨å…ƒæ•¸æ“š
ADAPTER_METADATA = {
    "name": "qwen3_8b_local_mcp",
    "display_name": "Qwen3 8B Local Model",
    "category": "ai_model",
    "version": "1.0.0",
    "description": "Qwen3 8Bæœ¬åœ°æ¨¡å‹é©é…å™¨ï¼Œæ”¯æŒWindows WSLã€macOSå’ŒLinux",
    "author": "PowerAutomation Team",
    "supported_platforms": ["windows_wsl", "macos", "linux"],
    "dependencies": ["ollama", "requests"],
    "config_schema": {
        "base_url": {
            "type": "string",
            "default": "http://localhost:11434",
            "description": "Ollamaæœå‹™åœ°å€"
        },
        "model_name": {
            "type": "string", 
            "default": "qwen2.5:8b",
            "description": "æ¨¡å‹åç¨±"
        }
    }
}

if __name__ == "__main__":
    # æ¸¬è©¦é©é…å™¨
    async def test_adapter():
        adapter = Qwen3LocalModelMCP()
        
        print("ğŸ§ª æ¸¬è©¦Qwen3 8Bé©é…å™¨...")
        
        # å¥åº·æª¢æŸ¥
        health = await adapter.health_check()
        print(f"å¥åº·æª¢æŸ¥: {health}")
        
        # åˆå§‹åŒ–
        if await adapter.initialize():
            print("âœ… åˆå§‹åŒ–æˆåŠŸ")
            
            # æ¸¬è©¦ç”Ÿæˆ
            result = await adapter.generate_response("ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
            print(f"ç”Ÿæˆæ¸¬è©¦: {result}")
            
            # æ¸¬è©¦èŠå¤©
            messages = [{"role": "user", "content": "ä»€éº¼æ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}]
            chat_result = await adapter.chat_completion(messages)
            print(f"èŠå¤©æ¸¬è©¦: {chat_result}")
        else:
            print("âŒ åˆå§‹åŒ–å¤±æ•—")
    
    asyncio.run(test_adapter())

