# powerauto.aicorenew/mcp/mcp_coordinator/test_cases.py

import asyncio
import json
import time
from typing import Dict, Any
from .shared_core_integration import create_shared_core_integrator, create_enhanced_dialog_processor
from .dialog_classifier import OneStepSuggestionGenerator

class TestCaseRunner:
    """
    æµ‹è¯•ç”¨ä¾‹è¿è¡Œå™¨
    ä¸“é—¨ç”¨äºè¿è¡Œæ™ºèƒ½ç¼–è¾‘å™¨å’ŒManusçš„ä¸¤æ¡æµ‹è¯•ç”¨ä¾‹
    """
    
    def __init__(self, architecture_type: str = "consumer"):
        self.integrator = create_shared_core_integrator(architecture_type)
        self.dialog_processor = create_enhanced_dialog_processor(self.integrator)
        self.suggestion_generator = OneStepSuggestionGenerator()

    async def initialize(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        await self.integrator.initialize_integration()

    async def run_smart_editor_test(self) -> Dict[str, Any]:
        """
        æµ‹è¯•ç”¨ä¾‹1: æ™ºèƒ½ç¼–è¾‘å™¨ä»‹å…¥æ“ä½œå¯¹è¯æ¡†æä¾›æ™ºèƒ½ä¸€æ­¥ç›´è¾¾å»ºè®®
        """
        print("ğŸ§  å¼€å§‹æµ‹è¯•ï¼šæ™ºèƒ½ç¼–è¾‘å™¨ä»‹å…¥æ“ä½œå¯¹è¯æ¡†")
        
        # æ¨¡æ‹Ÿæ™ºèƒ½ç¼–è¾‘å™¨å¯¹è¯æ¡†æ•°æ®
        test_scenarios = [
            {
                "dialog_content": "å¦‚ä½•ä¼˜åŒ–è¿™æ®µPythonä»£ç çš„æ€§èƒ½ï¼Ÿ",
                "context": {
                    "source": "smart_editor",
                    "session_id": "editor_test_001",
                    "file_type": "python",
                    "code_snippet": "for i in range(1000000): result.append(i**2)",
                    "current_line": 42
                },
                "expected_type": "thinking"
            },
            {
                "dialog_content": "æ£€æŸ¥å½“å‰æ–‡ä»¶çš„è¯­æ³•é”™è¯¯",
                "context": {
                    "source": "smart_editor", 
                    "session_id": "editor_test_002",
                    "file_type": "javascript",
                    "file_path": "/src/main.js"
                },
                "expected_type": "observing"
            },
            {
                "dialog_content": "è‡ªåŠ¨æ ¼å¼åŒ–å½“å‰ä»£ç ",
                "context": {
                    "source": "smart_editor",
                    "session_id": "editor_test_003", 
                    "file_type": "python",
                    "selection": "lines 10-50"
                },
                "expected_type": "action"
            }
        ]
        
        results = []
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nğŸ“ å­æµ‹è¯• {i}: {scenario['dialog_content']}")
            
            # ç”Ÿæˆå»ºè®®
            suggestion = await self.dialog_processor.process_dialog_with_logging(
                scenario["dialog_content"],
                scenario["context"]
            )
            
            # éªŒè¯ç»“æœ
            is_correct_type = suggestion.get("dialog_type") == scenario["expected_type"]
            confidence = suggestion.get("confidence", 0.0)
            
            result = {
                "test_id": f"smart_editor_{i}",
                "dialog_content": scenario["dialog_content"],
                "expected_type": scenario["expected_type"],
                "actual_type": suggestion.get("dialog_type"),
                "confidence": confidence,
                "suggestion": suggestion,
                "type_match": is_correct_type,
                "high_confidence": confidence > 0.7,
                "success": is_correct_type and confidence > 0.5
            }
            
            results.append(result)
            
            print(f"   âœ… ç±»å‹: {suggestion.get('dialog_type')} (æœŸæœ›: {scenario['expected_type']})")
            print(f"   ğŸ“Š ç½®ä¿¡åº¦: {confidence:.2f}")
            print(f"   ğŸ’¡ å»ºè®®: {suggestion.get('title', 'N/A')}")
            print(f"   ğŸ¯ æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["success"])
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        summary = {
            "test_name": "æ™ºèƒ½ç¼–è¾‘å™¨ä»‹å…¥æ“ä½œå¯¹è¯æ¡†",
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "results": results,
            "overall_success": success_rate >= 0.8
        }
        
        print(f"\nğŸ“Š æ™ºèƒ½ç¼–è¾‘å™¨æµ‹è¯•æ€»ç»“:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæ•°: {successful_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"   æ•´ä½“è¯„ä»·: {'âœ… é€šè¿‡' if summary['overall_success'] else 'âŒ éœ€è¦æ”¹è¿›'}")
        
        return summary

    async def run_manus_test(self) -> Dict[str, Any]:
        """
        æµ‹è¯•ç”¨ä¾‹2: Manusä»‹å…¥æ“ä½œå¯¹è¯æ¡†æä¾›æ™ºèƒ½ä¸€æ­¥ç›´è¾¾å»ºè®®
        """
        print("\nğŸŒ å¼€å§‹æµ‹è¯•ï¼šManusä»‹å…¥æ“ä½œå¯¹è¯æ¡†")
        
        # æ¨¡æ‹ŸManuså¯¹è¯æ¡†æ•°æ®
        test_scenarios = [
            {
                "dialog_content": "åˆ†æè¿™ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼å¹¶æä¾›ä¸ªæ€§åŒ–å»ºè®®",
                "context": {
                    "source": "manus",
                    "session_id": "manus_test_001",
                    "user_id": "user_12345",
                    "interaction_history": ["search", "click", "scroll"],
                    "current_page": "dashboard"
                },
                "expected_type": "thinking"
            },
            {
                "dialog_content": "æŸ¥çœ‹å½“å‰ç”¨æˆ·çš„æ´»è·ƒçŠ¶æ€",
                "context": {
                    "source": "manus",
                    "session_id": "manus_test_002",
                    "user_id": "user_67890",
                    "timestamp": time.time()
                },
                "expected_type": "observing"
            },
            {
                "dialog_content": "ä¸ºç”¨æˆ·åˆ›å»ºä¸ªæ€§åŒ–å·¥ä½œæµ",
                "context": {
                    "source": "manus",
                    "session_id": "manus_test_003",
                    "user_preferences": {"theme": "dark", "language": "zh-CN"},
                    "workflow_type": "automation"
                },
                "expected_type": "action"
            }
        ]
        
        results = []
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nğŸ¯ å­æµ‹è¯• {i}: {scenario['dialog_content']}")
            
            # ç”Ÿæˆå»ºè®®
            suggestion = await self.dialog_processor.process_dialog_with_logging(
                scenario["dialog_content"],
                scenario["context"]
            )
            
            # éªŒè¯ç»“æœ
            is_correct_type = suggestion.get("dialog_type") == scenario["expected_type"]
            confidence = suggestion.get("confidence", 0.0)
            
            result = {
                "test_id": f"manus_{i}",
                "dialog_content": scenario["dialog_content"],
                "expected_type": scenario["expected_type"],
                "actual_type": suggestion.get("dialog_type"),
                "confidence": confidence,
                "suggestion": suggestion,
                "type_match": is_correct_type,
                "high_confidence": confidence > 0.7,
                "success": is_correct_type and confidence > 0.5
            }
            
            results.append(result)
            
            print(f"   âœ… ç±»å‹: {suggestion.get('dialog_type')} (æœŸæœ›: {scenario['expected_type']})")
            print(f"   ğŸ“Š ç½®ä¿¡åº¦: {confidence:.2f}")
            print(f"   ğŸ’¡ å»ºè®®: {suggestion.get('title', 'N/A')}")
            print(f"   ğŸ¯ æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["success"])
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        summary = {
            "test_name": "Manusä»‹å…¥æ“ä½œå¯¹è¯æ¡†",
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "results": results,
            "overall_success": success_rate >= 0.8
        }
        
        print(f"\nğŸ“Š Manusæµ‹è¯•æ€»ç»“:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæ•°: {successful_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"   æ•´ä½“è¯„ä»·: {'âœ… é€šè¿‡' if summary['overall_success'] else 'âŒ éœ€è¦æ”¹è¿›'}")
        
        return summary

    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        print("ğŸš€ å¼€å§‹è¿è¡ŒPowerAutomationæ™ºèƒ½ä»‹å…¥æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        await self.initialize()
        
        # è¿è¡Œä¸¤æ¡ä¸»è¦æµ‹è¯•
        smart_editor_result = await self.run_smart_editor_test()
        manus_result = await self.run_manus_test()
        
        # ç»¼åˆè¯„ä¼°
        total_tests = smart_editor_result["total_tests"] + manus_result["total_tests"]
        total_successful = smart_editor_result["successful_tests"] + manus_result["successful_tests"]
        overall_success_rate = total_successful / total_tests if total_tests > 0 else 0
        
        final_result = {
            "test_suite": "PowerAutomationæ™ºèƒ½ä»‹å…¥æµ‹è¯•",
            "timestamp": time.time(),
            "smart_editor_test": smart_editor_result,
            "manus_test": manus_result,
            "overall_stats": {
                "total_tests": total_tests,
                "successful_tests": total_successful,
                "success_rate": overall_success_rate,
                "overall_pass": overall_success_rate >= 0.8
            }
        }
        
        print("\n" + "=" * 60)
        print("ğŸ† æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæ•°: {total_successful}")
        print(f"   æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.1%}")
        print(f"   æœ€ç»ˆè¯„ä»·: {'ğŸ‰ æµ‹è¯•é€šè¿‡ï¼' if final_result['overall_stats']['overall_pass'] else 'âš ï¸  éœ€è¦ä¼˜åŒ–'}")
        
        # å¥åº·æ£€æŸ¥
        health_status = await self.integrator.health_check()
        final_result["health_check"] = health_status
        
        return final_result

    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        await self.integrator.shutdown()

# CLIæ¥å£
async def main():
    """ä¸»å‡½æ•° - CLIå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="PowerAutomationæ™ºèƒ½ä»‹å…¥æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--test", choices=["all", "editor", "manus"], default="all",
                       help="é€‰æ‹©è¦è¿è¡Œçš„æµ‹è¯•")
    parser.add_argument("--architecture", choices=["enterprise", "consumer", "opensource"], 
                       default="consumer", help="æ¶æ„ç±»å‹")
    parser.add_argument("--output", help="è¾“å‡ºç»“æœåˆ°JSONæ–‡ä»¶")
    
    args = parser.parse_args()
    
    runner = TestCaseRunner(args.architecture)
    
    try:
        if args.test == "all":
            result = await runner.run_all_tests()
        elif args.test == "editor":
            await runner.initialize()
            result = await runner.run_smart_editor_test()
        elif args.test == "manus":
            await runner.initialize()
            result = await runner.run_manus_test()
        
        # è¾“å‡ºç»“æœ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        return result
        
    finally:
        await runner.cleanup()

if __name__ == "__main__":
    asyncio.run(main())

